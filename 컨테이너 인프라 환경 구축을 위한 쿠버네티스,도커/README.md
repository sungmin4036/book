
---

## ㅁ 오케스트레이션

---

컨테이너 인프라 환경이란 리눅스 운영 체제의 커널 하나에서 여러 개의 컨테이너가 격리된 상태로 실행되는 인프라 환경을 말합니다.

여기서 컨테이너는 하나 이상의 목적을 위해 독립적으로 작동하는 프로세스입니다.

개인 환경에서는 1명의 관리자(사용자)가 다양한 응용프로그램을 사용하므로 각각의 프로그램을 컨테이너로 구현할 필요가 거의 없습니다. 

하지만 기업 환경에서는 다수의 관리자가 수백 또는 수천 대의 서버를 함께 관리하기 때문에 일관성을 유지하는 것이 매우 중요합니다.

이런 경우 컨테이너 인프라 환경을 구성하면 눈송이 서버(여러 사람이 만져서 설정의 일관성이 떨어진 서버)를 방지하는 데 효과적입니다.

가상화 환경에서는 각각의 가상 머신이 모두 독립적인 운영 체제 커널을 가지고 있어야 하기 때문에 그만큼 자원을 더 소모해야 하고 성능이 떨어질 수밖에 없습니다. 

하지만 컨테이너 인프라 환경은 운영 체제 커널 하나에 컨테이너 여러 개가 격리된 형태로 실행되기 때문에 자원을 효율적으로 사용할 수 있고 거치는 단계가 적어서 속도도 훨씬 빠릅니다.

![image](https://user-images.githubusercontent.com/62640332/167587623-bb332b02-f6a2-45d1-8a2c-7d2e9c3aa874.png)

### ㅁ 컨테이너 인프라 환경이 주목 받지 못했던 이유

---

컨테이너 인프라 환경이 처음부터 주목받았던 것은 아닙니다. 

이미 가상화 환경에서 상용 솔루션(VMware)을 이용해 안정적으로 시스템을 운용하고 있었고, 기술 성숙도가 높아 문제없이 관리되고 있었습니다. 

그러다 시간이 지나 커널을 공유해 더 많은 애플리케이션을 올릴 수 있는 컨테이너가 도입되기 시작하면서 늘어난 컨테이너를 관리해야 했습니다. 

하지만 기존의 컨테이너 관리 솔루션(Docker Swarm, Mesos, Nomad 등)들은 현업의 요구 사항을 충족시키기에는 부족한 점이 있었습니다.

그래서 컨테이너 인프라 환경이 주는 장점이 많이 있음에도 컨테이너 관리 문제 때문에 보편화되기가 어려웠습니다.

그 이후 구글이 쿠버네티스를 오픈소스로 공개하여 컨테이너 관리가 용이해짐.


<br>
<br>

쿠버네티스를 컨테이너 관리 도구라고 설명했지만, 실제로 쿠버네티스는 컨테이너 오케스트레이션을 위한 솔루션입니다. 

오케스트레이션(Orchestration)이란 복잡한 단계를 관리하고 요소들의 유기적인 관계를 미리 정의해 손쉽게 사용하도록 서비스를 제공하는 것을 의미합니다. 

다수의 컨테이너를 유기적으로 연결, 실행, 종료할 뿐만 아니라 상태를 추적하고 보존하는 등 컨테이너를 안정적으로 사용할 수 있게 만들어주는 것이 컨테이너 오케스트레이션입니다

### ㅁ 대표적인 컨테이너 오케스트레이션

---

![image](https://user-images.githubusercontent.com/62640332/167589921-015c8a2b-9527-469a-b17c-709e8a5ce09c.png)


• 도커 스웜(Docker Swarm): 간단하게 설치할 수 있고 사용하기도 용이합니다. 

그러나 그만큼 기능이 다양하지 않아 대규모 환경에 적용하려면 사용자 환경을 변경해야 할 수 있습니다. 

따라서 소규모 환경에서는 유용하지만 대규모 환경에서는 잘 사용하지 않는 편입니다.

<br>
<br>

• 메소스(Mesos): 아파치(Apache)의 오픈 소스 프로젝트로 역사와 전통이 있는 클러스터 도구이며 트위터, 에어비앤비, 애플, 우버 등 다양한 곳에서 이미 검증된 솔루션입니다. 

메소스는 2016년 DC/OS(Data Center OS, 대규모 서버 환경에서 자원을 유연하게 공유하며 하나의 자원처럼 관리하는 도구)의 지원으로 매우 간결해졌습니다. 

하지만 기능을 충분히 활용하려면 분산 관리 시스템과 연동해야 합니다. 따라서 여러 가지 솔루션을 유기적으로 구성해야 하는 부담이 있습니다.

<br>
<br>

• 노매드(Nomad): 베이그런트를 만든 해시코프(HashiCorp)사의 컨테이너 오케스트레이션으로, 베이그런트처럼 간단한 구성으로 컨테이너 오케스트레이션 환경을 제공합니다. 

하지만 도커 스웜과 마찬가지로 기능이 부족하므로 복잡하게 여러 기능을 사용하는 환경이 아닌 가볍고 간단한 기능만 필요한 환경에서 사용하기를 권장합니다. 

해시코프의 Consul(서비스 검색, 구성 및 분할 기능 제공)과 Vault(암호화 저장소)와의 연동이 원할하므로 이런 도구에 대한 사용 성숙도가 높은 조직이라면 노매드 도입을 고려해볼 수 있습니다.

<br>
<br>


• 쿠버네티스: 다른 오케스트레이션 솔루션보다는 시작하는 데 어려움이 있지만, 쉽게 사용할 수 있도록 도와주는 도구들이 있어서 설치가 쉬워지는 추세입니다. 

또한 다양한 형태의 쿠버네티스가 지속적으로 계속 발전되고 있어서 컨테이너 오케스트레이션을 넘어 IT 인프라 자체를 컨테이너화하고, 컨테이너화된 인프라 제품군을 쿠버네티스 위에서 동작할 수 있게 만듭니다. 

즉 거의 모든 벤더와 오픈 소스 진영 모두에서 쿠버네티스를 지원하고 그에 맞게 통합 개발하고 있습니다. 

그러므로 컨테이너 오케이스트레이션을 학습하거나 도입하려고 한다면 쿠버네티스를 우선적으로 고려해야 합니다.

![image](https://user-images.githubusercontent.com/62640332/167590383-25eb6435-ace8-4906-ba6a-81b7dcaa280c.png)

<br>
<br>


### ㅁ 쿠버네티스 구성하는 방법

---

1. 퍼블릭 클라우드 업체에서 제공하는 관리형 쿠버네티스인 EKS(Amazon Elastic Kubernetes Service), AKS(Azure Kubernetes Services), GKE(Google Kubernetes Engine) 등을 사용합니다. 

구성이 이미 다 갖춰져 있고 마스터 노드를 클라우드 업체에서 관리하기 때문에 학습용으로는 적합하지 않습니다.
 
<br>

2. 수세의 Rancher, 레드햇의 OpenShift와 같은 플랫폼에서 제공하는 설치형 쿠버네티스를 사용합니다. 

하지만 유료라 쉽게 접근하기 어렵습니다.

<br>

3. 사용하는 시스템에 쿠버네티스 클러스터를 자동으로 구성해주는 솔루션을 사용합니다. 주요 솔루션으로는 kubeadm, kops(Kubernetes Operations), KRIB(Kubernetes Rebar Integrated Bootstrap), kubespray가 있습니다. 

4가지의 주요 솔루션 중에 kubeadm이 가장 널리 알려져 있습니다. kubeadm은 사용자가 변경하기도 수월하고, 온프레미스(On-Premises)와 클라우드를 모두 지원하며, 배우기도 쉽습니다. 

이러한 솔루션들을 `구성형 쿠버네티스`라고 합니다.

![image](https://user-images.githubusercontent.com/62640332/167782538-304d74f9-fea9-4be4-8a61-d12be53091b1.png)

<br>
<br>

```
Tip ☆ 쿠버네티스 구성 요소의 이름 생성 규칙
    
쿠버네티스의 구성 요소는 동시에 여러 개가 존재하는 경우 중복된 이름을 피하려고 뒤에 해시(hash) 코드가 삽입됩니다. 
이때 해시 코드는 무작위 문자열로 생성됩니다.
```

![image](https://user-images.githubusercontent.com/62640332/167785690-9ba5892d-0d5c-4961-8413-d14801b9243b.png)

coredns에는 중간에 5644d7b6d9라는 문자열이 하나 더 있는데, 이는 레플리카셋(ReplicaSet)을 무작위 문자열로 변형해 추가한 것입니다. calico-kube-controllers도 같은 경우입니다.

![image](https://user-images.githubusercontent.com/62640332/167785776-3098f442-68b2-45d6-a03b-566dcbf54bff.png)

<br>
<br>

- 관리자나 개발자가 파드를 배포할 때

![image](https://user-images.githubusercontent.com/62640332/167785874-a6a6f5c6-5737-409a-812f-830675e64add.png)

<br>
<br>

### ㅁ 마스터 노드

---

- kubectl: 쿠버네티스 클러스터에 명령을 내리는 역할을 합니다. 
 
다른 구성 요소들과 다르게 바로 실행되는 명령 형태인 바이너리(binary)로 배포되기 때문에 마스터 노드에 있을 필요는 없습니다.

하지만 통상적으로 API 서버와 주로 통신하므로 이 책에서는 API 서버가 위치한 마스터 노드에 구성했습니다.

<br>

➊ API 서버: 쿠버네티스 클러스터의 중심 역할을 하는 통로입니다. 

주로 상태 값을 저장하는 etcd와 통신하지만, 그 밖의 요소들 또한 API 서버를 중심에 두고 통신하므로 API 서버의 역할이 매우 중요합니다. 

회사에 비유하면 모든 직원과 상황을 관리하고 목표를 설정하는 관리자에 해당합니다.

<br>
 
➋ etcd: 구성 요소들의 상태 값이 모두 저장되는 곳입니다. 회사의 관리자가 모든 보고 내용을 기록하는 노트라고 생각하면 됩니다. 

실제로 etcd 외의 다른 구성 요소는 상태 값을 관리하지 않습니다. 

그러므로 etcd의 정보만 백업돼 있다면 긴급한 장애 상황에서도 쿠버네티스 클러스터는 복구할 수 있습니다. 

또한 etcd는 분산 저장이 가능한 key-value 저장소이므로, 복제해 여러 곳에 저장해 두면 하나의 etcd에서 장애가 나더라도 시스템의 가용성을 확보할 수 있습니다. 

이와 같은 멀티 마스터 노드 형태는 부록에서 kubespray로 구성해 보겠습니다.

```
Tip ☆ etcd의 의미

etcd(엣시디)를 약어로 오인하는 경우가 있습니다. 
etcd는 리눅스의 구성 정보를 주로 가지고 있는 etc 디렉터리와 distributed(퍼뜨렸다)의 합성어입니다.
따라서 etcd는 구성 정보를 퍼뜨려 저장하겠다는 의미입니다.
```  

<br>

➌ 컨트롤러 매니저: 컨트롤러 매니저는 쿠버네티스 클러스터의 오브젝트 상태를 관리합니다. 

예를 들어 워커 노드에서 통신이 되지 않는 경우, 상태 체크와 복구는 컨트롤러 매니저에 속한 노드 컨트롤러에서 이루어집니다. 

다른 예로 레플리카셋 컨트롤러는 레플리카셋에 요청받은 파드 개수대로 파드를 생성합니다. 

뒤에 나오는 서비스와 파드를 연결하는 역할을 하는 엔드포인트 컨트롤러 또한 컨트롤러 매니저입니다.

이와 같이 다양한 상태 값을 관리하는 주체들이 컨트롤러 매니저에 소속돼 각자의 역할을 수행합니다. 

여기서 나온 오브젝트에 관해서는 ‘3.2.2 오브젝트란’에서 자세히 다룹니다.

<br>

➍ 스케줄러: 노드의 상태와 자원, 레이블, 요구 조건 등을 고려해 파드를 어떤 워커 노드에 생성할 것인지를 결정하고 할당합니다. 

스케줄러라는 이름에 걸맞게 파드를 조건에 맞는 워커 노드에 지정하고, 파드가 워커 노드에 할당되는 일정을 관리하는 역할을 담당합니다.

<br>
<br>

### ㅁ 워커 노드

<br>

➎ kubelet: 파드의 구성 내용(PodSpec)을 받아서 컨테이너 런타임으로 전달하고, 파드 안의 컨테이너들이 정상적으로 작동하는지 모니터링합니다.

<br>

➏ 컨테이너 런타임(CRI, Container Runtime Interface): 파드를 이루는 컨테이너의 실행을 담당합니다. 

파드 안에서 다양한 종류의 컨테이너가 문제 없이 작동하게 만드는 표준 인터페이스입니다. 

자세한 내용은 ‘부록 D 컨테이너 깊게 들여다보기’를 참고하기 바랍니다.

<br>

➐ 파드(Pod): 한 개 이상의 컨테이너로 단일 목적의 일을 하기 위해서 모인 단위입니다. 

즉, 웹 서버 역할을 할 수도 있고 로그나 데이터를 분석할 수도 있습니다. 

여기서 중요한 것은 파드는 언제라도 죽을 수 있는 존재라는 점입니다. 

이것이 쿠버네티스를 처음 배울 때 가장 이해하기 어려운 부분입니다. 

가상 머신은 언제라도 죽을 수 있다고 가정하고 디자인하지 않지만, 파드는 언제라도 죽을 수 있다고 가정하고 설계됐기 때문에 쿠버네티스는 여러 대안을 디자인했습니다. 

어려운 내용이므로 여러 가지 테스트를 통해 여러분이 이해하도록 돕겠습니다.

  
    
 ![image](https://user-images.githubusercontent.com/62640332/167788296-b9fe3509-b218-4e15-8073-bd54ad910703.png)


<br>
<br>

### ㅁ 선택 가능한 구성 요소

0 번부터  7 번까지는 기본 설정으로 배포된 쿠버네티스에서 이루어지는 통신 단계를 구분한 것입니다. 

이외에 선택적으로 배포하는 것들은 순서와 상관이 없기 때문에 10번대로 구분해 표시했습니다. 

선택 가능한 부가 요소들은 이 책에서 다루기에는 너무 깊은 내용이라 이런 요소가 있다는 정도만 알면 충분합니다.

<br>

⓫ 네트워크 플러그인: 쿠버네티스 클러스터의 통신을 위해서 네트워크 플러그인을 선택하고 구성해야 합니다. 

네트워크 플러그인은 일반적으로 CNI로 구성하는데, 주로 사용하는 CNI에는 캘리코(Calico), 플래널(Flannel), 실리움(Cilium), 큐브 라우터(Kube-router), 로마나(Romana), 위브넷(WeaveNet), Canal이 있습니다. 여기서는 캘리코를 선택해 구성했습니다.

<br>

⓬ CoreDNS: 클라우드 네이티브 컴퓨팅 재단에서 보증하는 프로젝트로, 빠르고 유연한 DNS 서버입니다. 

쿠버네티스 클러스터에서 도메인 이름을 이용해 통신하는 데 사용하며, 6장에서 간단히 사용해 볼 예정입니다. 

실무에서 쿠버네티스 클러스터를 구성하여 사용할 때는 IP보다 도메인 네임을 편리하게 관리해 주는 CoreDNS를 사용하는 것이 일반적입니다. 

해당 내용을 자세히 알아보려면 홈페이지(https://coredns.io)를 참조하기 바랍니다.

```  
Tip ☆ CNI

CNI(Container Network Interface, 컨테이너 네트워크 인터페이스)는 클라우드 네이티브 컴퓨팅 재단의 프로젝트로,
컨테이너의 네트워크 안정성과 확장성을 보장하기 위해 개발됐습니다. 
CNI에 사용할 수 있는 네트워크 플러그인은 다양한데, 구성 방식과 지원하는 기능, 성능이 각기 다르므로 사용 목적에 맞게 선택하면 됩니다. 
예를 들어 Calico는 L3로 컨테이너 네트워크를 구성하고, Flannel은 L2로 컨테이너 네트워크를 구성합니다. 
또한 네트워크 프로토콜인 BGP와 VXLAN의 지원, ACL(Access Control List) 지원, 보안 기능 제공 등을 살펴보고 필요한 조건을 가지고 있는 네트워크 플러그인을 선택할 수 있어서 설계 유연성이 매우 높습니다.
```
<br>
<br>


### ㅁ 사용자가 배포된 파드에 접속할 때

---

1. kube-proxy: 쿠버네티스 클러스터는 파드가 위치한 노드에 kube-proxy를 통해 파드가 통신할 수 있는 네트워크를 설정합니다. 

이때 실제 통신은 br_netfilter와 iptables로 관리합니다. 

두 기능은 Vagrantfile에서 호출하는 config.sh 코드를 설명할 때 다뤘습니다.

<br>

2. 파드: 이미 배포된 파드에 접속하고 필요한 내용을 전달받습니다. 이때 대부분 사용자는 파드가 어느 워커 노드에 위치하는지 신경 쓰지 않아도 됩니다.

쿠버네티스의 각 구성 요소를 파드의 배포와 접속 관점에서 설명했지만, 이해하기는 쉽지 않을 겁니다. 

파드가 배포되는 과정을 살펴보며 쿠버네티스의 구성 요소를 좀 더 깊이 알아보겠습니다.

<br>

쿠버네티스의 가장 큰 장점은 쿠버네티스의 구성 요소마다 하는 일이 명확하게 구분돼 각자의 역할만 충실하게 수행하면 클러스터 시스템이 안정적으로 운영된다는 점입니다. 

이렇게 각자의 역할이 명확하게 나뉘어진 것은 마이크로서비스 아키텍처(MSA) 구조와도 밀접하게 연관됩니다. 

또한 역할이 나뉘어 있어서 문제가 발생했을 때 어느 부분에서 문제가 발생했는지 디버깅하기 쉽습니다.


<br>
<br>



### ㅁ 파드의 생명주기(life cycle)

---

![image](https://user-images.githubusercontent.com/62640332/167790485-22cbb381-5aa0-4d58-a373-b2bb744c3334.png)

1. kubectl을 통해 API 서버에 파드 생성을 요청합니다.

<br>

2. (업데이트가 있을 때마다 매번) API 서버에 전달된 내용이 있으면 API 서버는 etcd에 전달된 내용을 모두 기록해 클러스터의 상태 값을 최신으로 유지합니다. 
 
따라서 각 요소가 상태를 업데이트할 때마다 모두 API 서버를 통해 etcd에 기록됩니다.

<br>

3. API 서버에 파드 생성이 요청된 것을 컨트롤러 매니저가 인지하면 컨트롤러 매니저는 파드를 생성하고, 이 상태를 API 서버에 전달합니다. 

참고로 아직 어떤 워커 노드에 파드를 적용할지는 결정되지 않은 상태로 파드만 생성됩니다. 이 부분은 ‘3.2.2 오브젝트란’에서 보충 설명하겠습니다.

<br>

4. API 서버에 파드가 생성됐다는 정보를 스케줄러가 인지합니다. 스케줄러는 생성된 파드를 어떤 워커 노드에 적용할지 조건을 고려해 결정하고 해당 워커 노드에 파드를 띄우도록 요청합니다.

5. API 서버에 전달된 정보대로 지정한 워커 노드에 파드가 속해 있는지 스케줄러가 kubelet으로 확인합니다.

6. kubelet에서 컨테이너 런타임으로 파드 생성을 요청합니다.

7. 파드가 생성됩니다.

8. 파드가 사용 가능한 상태가 됩니다.

앞의 내용을 살펴보다가 ‘API 서버는 감시만 하는 걸까? 화살표가 반대로 그려져야 맞지 않을까?’라는 의문이 들었다면 내용을 제대로 본 겁니다.

이 부분은 쿠버네티스를 이해하는 데 매우 중요한 부분입니다. 쿠버네티스는 작업을 순서대로 진행하는 워크플로(workflow, 작업 절차) 구조가 아니라 `선언적인(declarative)` 시스템 구조를 가지고 있습니다. 

즉, 각 요소가 추구하는 상태(desired status)를 선언하면 현재 상태(current status)와 맞는지 점검하고 그것에 맞추려고 노력하는 구조로 돼 있다는 뜻입니다.

따라서 추구하는 상태를 API 서버에 선언하면 다른 요소들이 API 서버에 와서 현재 상태와 비교하고 그에 맞게 상태를 변경하려고 합니다. 

여기서 API는 현재 상태 값을 가지고 있는데, 이것을 보존해야 해서 etcd가 필요합니다. 

API 서버와 etcd는 거의 한몸처럼 움직이도록 설계됐습니다. 다만, 여기서 `워커 노드는 워크플로 구조`에 따라 설계됐습니다. 

쿠버네티스가 kubelet과 컨테이너 런타임을 통해 파드를 새로 생성하고 제거해야 하는 구조여서 선언적인 방식으로 구조화하기에는 어려움이 있기 때문입니다. 

또한 명령이 절차적으로 전달되는 방식은 시스템의 성능을 높이는 데 효율적입니다. 

하지만 마스터 노드는 이미 생성된 파드들을 유기적으로 연결하므로 쿠버네티스 `클러스터를 안정적으로 유지`하려면 선언적인 시스템이 더 낫습니다.

<br> 
<br> 

- kubectl

앞에서 kubectl은 꼭 마스터 노드에 위치할 필요 없다고 했습니다. 실제로 쿠버네티스 클러스터의 외부에서 쿠버네티스 클러스터에 명령을 내릴 수도 있습니다.

‘3.1.5 파드 생명주기로 쿠버네티스 구성 요소 살펴보기’를 보면 kubectl은 API 서버를 통해 쿠버네티스에 명령을 내립니다. 

따라서 kubectl이 어디에 있더라도 API 서버의 접속 정보만 있다면 어느 곳에서든 쿠버네티스 클러스터에 명령을 내릴 수 있습니다.

```
# 쿠버네티스 클러스터의 정보(/etc/kubernetes/admin.conf)
```

- kubelet

kubelet은 쿠버네티스에서 파드의 생성과 상태 관리 및 복구 등을 담당하는 매우 중요한 구성 요소입니다. 

따라서 kubelet에 문제가 생기면 파드가 정상적으로 관리되지 않습니다.

\# 기능을 검증하려면 실제로 파드를 배포해야 합니다.

- kube-proxy

kubelet이 파드의 상태를 관리한다면 kube-proxy는 파드의 통신을 담당합니다



### ㅁ 쿠버네티스 생성 명령어 run, create 차이점

---

run으로 파드를 생성하면 단일 파드 1개만 생성되고 관리됩니다. 

create deployment로 파드를 생성하면 디플로이먼트(Deployment)라는 관리 그룹 내에서 파드가 생성됩니다.

![image](https://user-images.githubusercontent.com/62640332/167969742-77ddc81e-e5b4-4abb-8e85-7d160390d239.png)

<br>
<br>

### ㅁ 오브젝트란

---

쿠버네티스를 사용하는 관점에서 파드와 디플로이먼트는 스펙(spec)과 상태(status) 등의 값을 가지고 있습니다. 

이러한 값을 가지고 있는 파드와 디플로이먼트를 개별 속성을 포함해 부르는 단위를 `오브젝트(Object)`라고 합니다.

<br>

1. 기본 오브젝트

• 파드(Pod): 쿠버네티스에서 실행되는 최소 단위, 즉 웹 서비스를 구동하는 데 필요한 최소 단위입니다. 

독립적인 공간과 사용 가능한 IP를 가지고 있습니다. 

하나의 파드는 1개 이상의 컨테이너를 갖고 있기 때문에 여러 기능을 묶어 하나의 목적으로 사용할 수도 있습니다. 

그러나 범용으로 사용할 때는 대부분 1개의 파드에 1개의 컨테이너를 적용합니다(차이가 조금 있으나 우선 1개라고 이해하겠습니다. 자세한 것은 4장에서 다룹니다).

<br>

• 네임스페이스(Namespaces): 쿠버네티스 클러스터에서 사용되는 리소스들을 구분해 관리하는 그룹입니다. 

예를 들어 3장에서는 3가지 네임스페이스를 사용합니다. 

> -  default: 특별히 지정하지 않으면 기본으로 할당
> -  kube-system: 쿠버네티스 시스템에서 사용되는 
> - metallb-system: 온프레미스에서 쿠버네티스를 사용할 경우 외부에서 쿠버네티스 클러스터 내부로 접속하게 도와주는 컨테이너들이 속해있음 

<br>

• 볼륨(Volume): 파드가 생성될 때 파드에서 사용할 수 있는 디렉터리를 제공합니다. 

기본적으로 파드는 영속되는 개념이 아니라 제공되는 디렉터리도 임시로 사용합니다. 

하지만 파드가 사라지더라도 저장과 보존이 가능한 디렉터리를 볼륨 오브젝트를 통해 생성하고 사용할 수 있습니다.

<br>

• 서비스(Service): 파드는 클러스터 내에서 유동적이기 때문에 접속 정보가 고정일 수 없습니다. 

따라서 파드 접속을 안정적으로 유지하도록 서비스를 통해 내/외부로 연결됩니다. 

그래서 서비스는 새로 파드가 생성될 때 부여되는 새로운 IP를 기존에 제공하던 기능과 연결해 줍니다. 

쉽게 설명하면 쿠버네티스 외부에서 쿠버네티스 내부로 접속할 때 내부가 어떤 구조로 돼 있는지, 파드가 살았는지 죽었는지 신경 쓰지 않아도 이를 논리적으로 연결하는 것이 서비스입니다. 

기존 인프라에서 로드밸런서, 게이트웨이와 비슷한 역할을 합니다. 서비스라는 이름 때문에 처음에 개념을 이해하기가 매우 어렵습니다. 

따라서 ‘3.3 쿠버네티스 연결을 담당하는 서비스’에서 집중적으로 다루겠습니다.

![image](https://user-images.githubusercontent.com/62640332/167970237-b6cd85ec-c8f5-4bf3-aeb5-8966f379a139.png)


<br>
<br>


2. 디플로이먼트

기본 오브젝트만으로도 쿠버네티스를 사용할 수 있습니다. 하지만 한계가 있어서 이를 좀 더 효율적으로 작동하도록 기능들을 조합하고 추가해 구현한 것이 디플로이먼트(Deployment)입니다

쿠버네티스에서 가장 많이 쓰이는 디플로이먼트 오브젝트는 파드에 기반을 두고 있으며, 레플리카셋 오브젝트를 합쳐 놓은 형태입니다

![image](https://user-images.githubusercontent.com/62640332/167970435-f843a24b-57ad-448a-a1ac-40d4edbbd10d.png)

실제로 API 서버와 컨트롤러 매니저는 단순히 파드가 생성되는 것을 감시하는 것이 아니라 디플로이먼트처럼 레플리카셋을 포함하는 오브젝트의 생성을 감시합니다

![image](https://user-images.githubusercontent.com/62640332/167970534-d0591fc1-1277-41d0-a1be-a1c1d16cf041.png)


### ㅁ 디플로이먼트가 필요한 이유

---

많은 사용자를 대상으로 웹 서비스를 하려면 다수의 파드가 필요한데, 이를 하나씩 생성한다면 매우 비효율적입니다. 

그래서 쿠버네티스에서는 `다수의 파드를 만드는 레플리카셋 오브젝트를 제공`합니다.

예를 들어 파드를 3개 만들겠다고 레플리카셋에 선언하면 컨트롤러 매니저와 스케줄러가 워커 노드에 파드 3개를 만들도록 선언합니다. 

![image](https://user-images.githubusercontent.com/62640332/167971781-46ca36a1-d036-4e8f-a78a-3a6afad8c445.png)

그러나 레플리카셋은 파드 수를 보장하는 기능만 제공하기 때문에 롤링 업데이트 기능 등이 추가된 `디플로이먼트를 사용해 파드 수를 관리하기를 권장`합니다

<br>

kubectl create deployment 명령으로 디플로이먼트를 생성하긴 했지만, 1개의 파드만 만들어졌을 뿐입니다. 

- 디플로이먼트를 생성하면서 한꺼번에 여러 개의 파드를 만들 순 없을까요? 

create에서는 replicas 옵션을 사용할 수 없고, scale은 이미 만들어진 디플로이먼트에서만 사용할 수 있습니다.

이런 설정을 적용하려면 필요한 내용을 파일로 작성해야 합니다. 

이때 작성하는 파일을 오브젝트 스펙(spec)이라고 합니다. 오브젝트 스펙은 일반적으로 야믈(YAML) 문법으로 작성합니다.

![image](https://user-images.githubusercontent.com/62640332/168002125-fb2b7a76-a1f7-44f2-8cfe-b45460f9b494.png)

> - run은 파드를 간단하게 생성하는 매우 편리한 방법입니다. 
하지만 run으로는 단일 파드만을 생성할 수 있습니다. 따라서 run을 모든 상황에 적용해 사용하기는 어렵습니다. 
> - create로 디플로이먼트를 생성하면 앞에서 확인한 것처럼 파일의 변경 사항을 바로 적용할 수 없다는 단점이 있습니다. 
이런 경우를 위해 쿠버네티스는 `apply`라는 명령어를 제공합니다.

![image](https://user-images.githubusercontent.com/62640332/168003524-a104f41a-e1aa-4b40-96cd-1e17db518e65.png)

<br>
<br>

쿠버네티스는 거의 모든 부분이 자동 복구되도록 설계됐습니다. 

특히 파드의 자동 복구 기술을 `셀프 힐링(Self-Healing)`이라고 하는데, 

제대로 작동하지 않는 컨테이너를 다시 시작하거나 교체해 파드가 정상적으로 작동하게 합니다

![image](https://user-images.githubusercontent.com/62640332/168007254-7db95912-d9a9-4944-a5ca-7be6fec4d90b.png)

디플로이먼트에 속한 파드가 삭제되면 레플리카셋이 확인하여 새로운 파드를 생성하여 설정한 replicas의 개수를 유지하지만,

디플로이에 속하지 않은 일반 파드 삭제시 어떤 컨트롤러도 이 파드를 관리 하지 안항서 그냥 삭제되고, 다시 생성X


```
# 디플로이먼트에 속한 파드는 상위 디플로이먼트를 삭제해야 파드가 삭제해야한다.
```

<br>
<br>


노드는 어떤 식으로 관리할까요? 우선 노드의 목적을 명확히 해야 합니다. 

노드는 `쿠버네티스 스케줄러에서 파드를 할당받고 처리하는 역할`을 합니다.

그런데 최근에 몇 차례 문제가 생긴 노드에 파드를 할당하면 문제가 생길 가능성이 높습니다. 

하지만 어쩔 수 없이 해당 노드를 사용해야 한다면 어떻게 할까요? 

이런 경우에는 영향도가 적은 파드를 할당해 일정 기간 사용하면서 모니터링해야 합니다. 

즉, 노드에 문제가 생기더라도 파드의 문제를 최소화해야 합니다. 

하지만 쿠버네티스는 모든 노드에 균등하게 파드를 할당하려고 합니다. 

그렇다면 어떻게 문제가 생길 가능성이 있는 노드라는 것을 쿠버네티스에 알려줄까요?

쿠버네티스에서는 이런 경우에 `cordon` 기능을 사용합니다.(해제 명령어는 `uncordon`)

![image](https://user-images.githubusercontent.com/62640332/168013662-f0fb55bc-e821-4608-afea-4f0a03c60ca3.png)

사용시 해당 노드의 상태가  `더이상 파드가 할당되지 않는 상태`로 변경. 파드수를 늘리든, 줄이든 cordon 설정한 노드는 변경되지 않음.

<br>
<br>

### ㅁ 노드의 커널을 업데이트하거나 노드의 메모리를 증설하는 등의 작업이 필요해서 노드를 꺼야 할 때는 어떻게 하면 좋을까요?

---

쿠버네티스를 사용하다 보면 정기 또는 비정기적인 유지보수를 위해 노드를 꺼야 하는 상황이 발생합니다. 

이런 경우를 대비해 쿠버네티스는 `drain` 기능을 제공합니다.

drain은 지정된 노드의 파드를 전부 다른 곳으로 이동시켜 해당 노드를 유지보수할 수 있게 합니다. 

drain은 실제로 파드를 옮기는 것이 아니라 노드에서 파드를 삭제하고 다른 곳에 다시 생성합니다. 

앞에서도 설명했지만 파드는 언제라도 삭제할 수 있기 때문에 쿠버네티스에서 대부분 이동은 파드를 지우고 다시 만드는 과정을 의미합니다. 

그런데 DaemonSet은 각 노드에 1개만 존재하는 파드라서 drain으로는 삭제할 수 없습니다.

```
# drain 명령과 ignore-daemonsets 옵션을 함께 사용합니다. 이 옵션을 사용하면 DaemonSet을 무시하고 진행
```

![image](https://user-images.githubusercontent.com/62640332/168028896-68a2e571-ffe5-4058-b68a-c926ea542f57.png)


![image](https://user-images.githubusercontent.com/62640332/168030734-afd386df-6951-4b89-abe0-2b304c1dc2a6.png)


<br>
<br>

### ㅁ 쿠버네티스 연결을 담당하는 서비스

---

일반적으로 서비스라고 하면 웹 서비스나 네트워크 서비스처럼 운영 체제에 속한 서비스 데몬 또는 개발 중인 서비스 등을 떠올릴 겁니다. 

그런데 쿠버네티스에서는 외부에서 쿠버네티스 클러스터에 접속하는 방법을 `서비스(service)`라고 합니다. 

서비스를 ‘소비를 위한 도움을 제공한다’는 관점으로 바라본다면 쿠버네티스가 외부에서 쿠버네티스 클러스터에 접속하기 위한 ‘서비스’를 제공한다고 볼 수 있습니다.


### ㅁ 노드포트

---

외부에서 쿠버네티스 클러스터의 내부에 접속하는 가장 쉬운 방법은 `노드포트(NodePort) 서비스`를 이용하는 것입니다. 

노드포트 서비스를 설정하면 모든 워커 노드의 특정 포트(노드포트)를 열고 여기로 오는 `모든 요청을 노드포트 서비스로 전달`합니다. 

그리고 노드포트 서비스는 해당 업무를 처리할 수 있는 `파드로 요청을 전달`합니다.

![image](https://user-images.githubusercontent.com/62640332/168031237-60b0163c-b62a-45a0-a3a5-3976c9bcdd61.png)

<br>

- 노드포트 자동으로 부하분산 되는 이유? 노드포트의 오브젝트 스펙에 적힌 np-pods와 디플로이먼트의 이름을 확인해 동일하면 같은 파드라고 간주하기 때문입니다

- 노드포트 생성방법] 오브젝트 스펙 파일, expose 명령어 사용

```
expose 명령어 실행시 포트번호 임의로 지정 불가능하며, 포트번호는 30000~32767에서 임의로 지정됨
```

### ㅁ 사용 목적별로 연결하는 인그레스

---

노드포트 서비스는 포트를 중복 사용할 수 없어서 1개의 노드포트에 1개의 디플로이먼트만 적용됩니다. 

그렇다면 여러 개의 디플로이먼트가 있을 때 그 수만큼 노드포트 서비스를 구동해야 할까요? 쿠버네티스에서는 이런 경우에 `인그레스`를 사용합니다. 

인그레스(Ingress)는 `고유한 주소를 제공`해 `사용 목적에 따라 다른 응답을 제공`할 수 있고, 트래픽에 대한 `L4/L7 로드밸런서`와 `보안 인증서`를 처리하는 기능을 제공합니다.

인그레스를 사용하려면 `인그레스 컨트롤러`가 필요합니다

여기서는 NGINX 인그레스 컨트롤러가 다음 단계로 작동합니다.

1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속합니다. 이때 노드포트 서비스를 NGINX 인그레스 컨트롤러로 구성합니다.

2. NGINX 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공합니다.

3. 클러스터 IP 서비스는 사용자를 해당 파드로 연결해 줍니다.

```
인그레스 컨트롤러는 파드와 직접 통신할 수 없어서 노드포트 또는 로드밸런서 서비스와 연동되어야 합니다. 따라서 노드포트로 이를 연동했습니다.
```

인그레스를 위한 설정 파일은 다음과 같습니다. 이 파일은 들어오는 주소 값과 포트에 따라 노출된 서비스를 연결하는 역할을 설정합니다.

- expose 명령으로 디플로이먼트(in-hname-pod, in-ip-pod)도 서비스로 노출합니다. 

외부와 통신하기 위해 클러스터 내부에서만 사용하는 파드를 클러스터 외부에 노출할 수 있는 구역으로 옮기는 것입니다. 

내부와 외부 네트워크를 분리해 관리하는 DMZ(DeMilitarized Zone, 비무장지대)와 유사한 기능입니다.

<br>
<br>

### ㅁ 클라우드에서 쉽게 구성 가능한 로드밸런서

---

앞에서 배운 연결 방식은 들어오는 요청을 모두 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고 이를 다시 쿠버네티스의 파드로 보내는 구조였습니다. 

이 방식은 매우 비효율적입니다. 그래서 쿠버네티스에서는 `로드밸런서(LoadBalancer)`라는 서비스 타입을 제공해 다음 그림과 같은 간단한 구조로 `파드를 외부에 노출하고 부하를 분산`합니다.

![image](https://user-images.githubusercontent.com/62640332/168427078-0a4c5ce2-ef19-40ad-bef4-02f028dd33b9.png)


- 왜 지금까지 로드밸런서를 사용하지 않았을까요? 

: 로드밸런서를 사용하려면 로드밸런서를 이미 구현해 둔 서비스업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야 하기 때문입니다. 

클라우드에서 제공하는 쿠버네티스를 사용하고 있다면 다음과 같이 선언만 하면 됩니다.

그러면 쿠버네티스 클러스터에 로드밸런서 서비스가 생성돼 외부와 통신할 수 있는 IP(EXTERNAL-IP)가 부여되고 외부와 통신할 수 있으며 부하도 분산됩니다.

<br>

### ㅁ MetalLB

---

: 온프레미스에서 로드밸런서를 사용하려면 내부에 로드밸런서 서비스를 받아주는 구성이 필요한데, 이를 지원하는 것이 `MetalLB`입니다. 

MetalLB는 베어메탈(bare metal, 운영 체제가 설치되지 않은 하드웨어)로 구성된 쿠버네티스에서도 로드밸런서를 사용할 수 있게 고안된 프로젝트입니다. 

MetalLB는 특별한 네트워크 설정이나 구성이 있는 것이 아니라 기존의 L2 네트워크(ARP/NDP)와 L3 네트워크(BGP)로 로드밸런서를 구현합니다.

![image](https://user-images.githubusercontent.com/62640332/168427259-10ec0bee-c52d-4295-8f5a-21a5d1c44f3c.png)

MetalLB 컨트롤러는 작동 방식(Protocol, 프로토콜)을 정의하고 EXTERNAL-IP를 부여해 관리합니다. 

MetalLB 스피커(speaker)는 정해진 작동 방식(L2/ARP, L3/BGP)에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공합니다. 

이때 L2는 스피커 중에서 리더를 선출해 경로 제공을 총괄하게 합니다.

<br>
<br>

### ㅁ HPA

---

부하량에 따라 디플로이먼트의 파드 수를 유동적으로 관리하는 기능을 제공합니다. 이를 HPA(Horizontal Pod Autoscaler)라고 합니다.

<br>
<br>

### ㅁ 데몬셋

---

데몬셋은 디플로이먼트의 replicas가 노드 수만큼 정해져 있는 형태라고 할 수 있는데, 노드 하나당 파드 한 개만을 생성합니다.

데몬셋은 언제 사용할까요? 사실 데몬셋은 이미 여러 번 사용했습니다. 

Calico 네트워크 플러그인과 kube-proxy를 생성할 때 사용했고, MetalLB의 스피커에서도 사용했습니다. 

이들의 공통점은 노드의 단일 접속 지점으로 노드 외부와 통신하는 것입니다. 따라서 파드가 1개 이상 필요하지 않습니다. 

결국 노드를 관리하는 파드라면 데몬셋으로 만드는 게 가장 효율적입니다.

<br>
<br>

### ㅁ 컨피그 맵

---

설정(config)을 목적으로 사용하는 오브젝트입니다. MetalLB를 구성할 때 컨피그맵을 사용해 봤습니다. 

인그레스에서는 설정을 위해 오브젝트를 인그레스로 선언했는데, 왜 MetalLB에서는 컨피그맵을 사용했을까요? 

명확하게 규정하기는 어려운데 인그레스는 오브젝트가 인그레스로 지정돼 있지만, 

MetalLB는 프로젝트 타입으로 정해진 오브젝트가 없어서 범용 설정으로 사용되는 컨피그맵을 지정했습니다.

<br>
<br>

### ㅁ PV와 PVC

---

이제 파드가 언제라도 생성되고 지워진다는 것을 충분히 알았을 것입니다. 쿠버네티스에서 의도적으로 이렇게 구현했습니다. 

그런데 때때로 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있습니다.


- 임시: emptyDir

- 로컬: host Path, local

- 원격: persistentVolumeClaim, cephfs, cinder, csi, fc(fibre channel), flexVolume, flocker, glusterfs, iscsi, nfs, portworxVolume, quobyte, rbd, scaleIO, storageos, vsphereVolume

- 특수 목적: downwardAPI, configMap, secret, azureFile, projected

- 클라우드: awsElasticBlockStore, azureDisk, gcePersistentDisk


쿠버네티스는 필요할 때 `PVC(PersistentVolumeClaim, 지속적으로 사용 가능한 볼륨 요청)`를 요청해 사용합니다. 

PVC를 사용하려면 `PV(PersistentVolume, 지속적으로 사용 가능한 볼륨)`로 볼륨을 선언해야 합니다. 간단하게 PV는 볼륨을 사용할 수 있게 준비하는 단계이고, PVC는 준비된 볼륨에서 일정 공간을 할당받는 것입니다. 

<br>

ㅁ PV로 볼륨을 선언할 수 있는 타입

![image](https://user-images.githubusercontent.com/62640332/168436459-bdb91b50-f0c6-4f30-a61d-60788c864e96.png)


PVC는 PV와 구성이 거의 동일합니다.

- PVC는 사용자(개발자)간 볼륨을 요청하는 데 사용
- PV는 사용자가 요청할 볼륨 공간을 관리자가 만듬

![image](https://user-images.githubusercontent.com/62640332/168439036-6ddc59b4-29b4-425a-a1d5-7881ca3a9810.png)

 PV와 PVC를 구성해서 PV와 PVC를 구성하는 주체가 관리자와 사용자로 나뉜다는 것을 확인했습니다. 
 
 또한 관리자와 사용자가 나뉘어 있지 않다면 굳이 PV와 PVC를 통하지 않고 바로 파드에 공유가 가능한 NFS 볼륨을 마운트할 수 있음을 알았습니다.

<br>
<br>

### ㅁ 스테이트풀셋

---

지금까지는 파드가 replicas에 선언된 만큼 무작위로 생성될 뿐이었습니다. 그런데 파드가 만들어지는 이름과 순서를 예측해야 할 때가 있습니다. 

주로 레디스(Redis), 주키퍼(Zookeeper), 카산드라(Cassandra), 몽고DB(MongoDB) 등의 마스터-슬레이브 구조 시스템에서 필요합니다.

`스테이트풀셋(StatefulSet)`을 사용합니다. 

스테이트풀셋은 volumeClaimTemplates 기능을 사용해 PVC를 자동으로 생성할 수 있고, 각 파드가 순서대로 생성되기 때문에 `고정된 이름, 볼륨, 설정 등`을 가질 수 있습니다. 

그래서 StatefulSet(이전 상태를 기억하는 세트)이라는 이름을 사용합니다. 다만, 효율성 면에서 좋은 구조가 아니므로 요구 사항에 맞게 적절히 사용하는 것이 좋습니다.

```
statefulset은 expose를 지원하지 않기떄문에, 로드밸런서 서비스를 실행해야합니다.

expose 명령으로 서비스를 생성할수 있는 오브젝트는 디플로이먼트, 파드, 레플리카셋, 레플리케이션 컨트롤러입니다.
```

- 스테이트풀셋은 헤드리스(Headless) 서비스로 노출한다고 하던데요?
    
네, 일반적으로는 맞습니다. 헤드리스 서비스는 `IP를 가지지 않는 서비스 타입`으로 중요한 자원인 IP를 절약할 수 있을 뿐만 아니라, 

스테이트풀셋과 같은 상태를 가지고 있는 오브젝트를 모두 노출하지 않고 상태 값을 외부에 알리고 싶은 것만 `선택적으로 노출`하게 할 수 있습니다. 

따라서 일반적으로는 스테이트풀셋은 헤드리스 서비스로 노출하나, IT에는 정답이 없듯 고정된 이름을 사용하면서 외부에 모든 스테이트풀셋을 노출하고자 하는 경우에는 노드포트나 로드밸런서 서비스로 노출할 수 있습니다. 

현재의 구성에서 헤드리스 서비스로 노출하고자 하는 경우에는 다음 코드를 사용해 노출할 수 있습니다.

노출된 IP는 없지만 내부적으로 각 파드의 이름과 노출된 서비스 이름등을 조합한 도메인 이름으로 아래와 같이 쿠버네티스 클러스터 내에서 통신할 수 있는 상태가 됩니다. 

이를 가능하게 해주는 CoreDNS는 6장 ‘쿠버네티스 내에서 도메인 이름을 제공하는 CoreDNS’를 참조하기 바랍니다.


## ㅁ 도커

---

파드들은 워커 노드라는 노드 단위로 관리하며, 

워커 노드와 마스터 노드가 모여 쿠버네티스 클러스터가 됩니다. 

그리고 파드는 1개 이상의 컨테이너로 이루어져 있습니다.

파드는 쿠버네티스로부터 IP를 받아 컨테이너가 외부와 통신할 수 있는 경로를 제공합니다. 

그리고 컨테이너들이 정상적으로 작동하는지 확인하고 네트워크나 저장 공간을 서로 공유하게 합니다. 

파드가 이러한 환경을 만들기 때문에 컨테이너들은 마치 하나의 호스트에 존재하는 것처럼 작동할 수 있습니다. 

정리하면, 컨테이너를 돌보는 것이 파드고, 

파드를 돌보는 것이 쿠버네티스 워커 노드이며, 

워커 노드를 돌보는 것이 쿠버네티스 마스터입니다. 

그런데 쿠버네티스 마스터 역시 파드(컨테이너)로 이루어져 있습니다.

![image](https://user-images.githubusercontent.com/62640332/168440490-8e24dc77-90e5-4fbf-996e-8801806d6f61.png)

이 구조를 이루는 가장 기본인 컨테이너는 `하나의 운영 체제` 안에서 `커널을 공유`하며 `개별적인 실행 환경을 제공`하는 격리된 공간입니다. 

여기서 개별적인 실행 환경이란 CPU, 네트워크, 메모리와 같은 시스템 자원을 독자적으로 사용하도록 할당된 환경을 말합니다. 

개별적인 실행 환경에서는 실행되는 프로세스를 구분하는 ID도 컨테이너 안에 격리돼 관리됩니다. 

그래서 각 컨테이너 내부에서 실행되는 애플리케이션들은 서로 영향을 미치지 않고 독립적으로 작동할 수 있습니다.

![image](https://user-images.githubusercontent.com/62640332/168440511-8d13d7b4-becb-47ab-a339-6cc8fade9c52.png)

각 컨테이너가 독립적으로 작동하기 때문에 여러 컨테이너를 효과적으로 다룰 방법이 필요해졌습니다. 

오래전부터 유닉스나 리눅스는 하나의 호스트 운영 체제 안에서 자원을 분리해 할당하고, 실행되는 프로세스를 격리해서 관리하는 방법을 제공했습니다. 

하지만 파일 시스템을 설정하고 자원과 공간을 관리하는 등의 복잡한 과정을 직접 수행해야 해서 일부 전문가만 사용할 수 있다는 단점이 있었습니다. 

이런 복잡한 과정을 쉽게 만들어 주는 도구로 등장한 것이 `도커`입니다. 

도커는 `컨테이너를 사용하는 방법을 명령어로 정리한 것`이라고 보면 됩니다. 

도커를 사용하면 사용자가 따로 신경 쓰지 않아도 컨테이너를 생성할 때 개별적인 실행 환경을 분리하고 자원을 할당합니다.

![image](https://user-images.githubusercontent.com/62640332/168440558-1d242c73-ed58-4501-a877-eea07bb00630.png)

쿠버네티스 인프라를 구성하는 데 가장 적합한 도구로 도커를 선택

<br>

베이그런트 이미지는 이미지 자체로는 사용할 수 없고 베이그런트를 실행할 때 추가해야만 사용할 수 있습니다. 

이와 마찬가지로 컨테이너 이미지도 그대로는 사용할 수 없고 도커와 같은 CRI로 불러들여야 컨테이너가 실제로 작동합니다. 

이는 실행 파일과 실행된 파일 관계로 볼 수 있습니다. 

따라서 컨테이너를 삭제할 때는 내려받은 이미지와 이미 실행된 컨테이너를 모두 삭제해야만 디스크의 용량을 온전히 확보할 수 있습니다.

1. 이미지 찾기
2. 실행하기
3. 디렉터리와 연결하기
4. 삭제하기


### 1. 이미지 검색

---

이미지는 레지스트리(registry)라고 하는 저장소에 모여 있습니다. 

레지스트리는 도커 허브(https://hub.docker.com)처럼 공개된 유명 레지스트리일 수도 있고, 내부에 구축한 레지스트리일 수도 있습니다. 

```
이미지 찾기
docker search <검색어>
```

![image](https://user-images.githubusercontent.com/62640332/168469659-7009be19-8f04-42de-a919-c7b0981bb170.png)

• INDEX: 이미지가 저장된 레지스트리의 이름입니다.

• NAME: 검색된 이미지 이름입니다. 공식 이미지를 제외한 나머지는 ‘레지스트리 주소/저장소 소유자/이미지 이름’ 형태입니다.

• DESCRIPTION: 이미지에 대한 설명입니다.

• STARS: 해당 이미지를 내려받은 사용자에게 받은 평가 횟수입니다. 사용자가 좋은 평가를 주고 싶을 때 스타(STAR)를 추가합니다. 숫자가 클수록 신뢰성 높은 이미지일 수 있습니다.

• OFFICIAL: [OK] 표시는 해당 이미지에 포함된 애플리케이션, 미들웨어 등을 개발한 업체에서 공식적으로 제공한 이미지라는 의미입니다.

• AUTOMATED: [OK] 표시는 도커 허브에서 자체적으로 제공하는 이미지 빌드 자동화 기능을 활용해 생성한 이미지를 의미합니다.

<br>

```
이미지 다운
docker pull <검색어>
```

<br>

![image](https://user-images.githubusercontent.com/62640332/168469735-97608036-7600-44ae-b99c-4f06970d6b3f.png)

• 태그(tag): Using default tag와 함께 뒤에 따라오는 태그 이름을 통해 이미지를 내려받을 때 사용한 태그를 알 수 있습니다. 

아무런 조건을 주지 않고 이미지 이름만으로 pull을 수행하면 기본으로 latest 태그가 적용됩니다. 

latest 태그는 가장 최신 이미지를 의미합니다. 따라서 내려받는 이미지 버전이 다를 수 있습니다.

<br>

• 레이어(layer): d121f8d1c412, ebd81fc8c071, 655316c160af, d15953c0e0f8, 2ee525c5c3cc는 pull을 수행해 내려받은 레이어입니다. 

하나의 이미지는 여러 개의 레이어로 이루어져 있어서 레이어마다 Pull complete 메시지가 발생합니다.

<br>

• 다이제스트(digest): 이미지의 고유 식별자로, 이미지에 포함된 내용과 이미지의 생성 환경을 식별할 수 있습니다. 

식별자는 해시(hash) 함수로 생성되며 이미지가 동일한지 검증하는 데 사용합니다. 이름이나 태그는 이미지를 생성할 때 임의로 지정하므로 이름이나 태그가 같다고 해서 같은 이미지라고 할 수 없습니다. 

그러나 다이제스트는 고유한 값이므로 다이제스트가 같은 이미지는 이름이나 태그가 다르더라도 같은 이미지입니다.

<br>

• 상태(Status): 이미지를 내려받은 레지스트리, 이미지, 태그 등의 상태 정보를 확인할 수 있습니다. 

형식은 ‘레지스트리 이름/이미지 이름:태그’입니다. 여기서는 내려받은 이미지는 docker.io 레지스트리에서 왔으며, 

이미지의 이름은 nginx, 태그는 앞서 설명한 것처럼 별도의 태그를 지정하지 않았기 때문에 기본 태그인 latest입니다.

<br>
<br>

#### ㅁ 이미지 태그


태그는 이름이 동일한 이미지에 추가하는 식별자입니다. 

이름이 동일해도 도커 이미지의 버전이나 플랫폼(CPU 종류나 기본 베이스를 이루는 운영 체제 등)이 다를 수 있기 때문에 이를 구분하는 데 사용합니다. 

이미지를 내려받거나 이미지를 기반으로 컨테이너를 구동할 때는 이미지 이름만 사용하고 태그를 명시하지 않으면 latest 태그를 기본으로 사용합니다. 

이미지 태그와 관련된 정보는 해당 이미지의 도커 허브 메뉴 중 Tags 탭에서 확인할 수 있습니다.

<br>
<br>

#### ㅁ 이미지의 레이어 구조


앞에서 컨테이너 이미지는 실행 파일이라고 했는데, 사실 이미지는 애플리케이션과 각종 파일을 담고 있다는 점에서 ZIP 같은 압축 파일에 더 가깝습니다.

그런데 압축 파일은 압축한 파일의 개수에 따라 전체 용량이 증가합니다. 하지만 이미지는 같은 내용일 경우 여러 이미지에 동일한 레이어를 공유하므로 전체 용량이 감소합니다.

다음 그림은 내부에 동일한 파일이 포함된 압축 파일과 이미지를 보여줍니다. 압축 파일은 내용이 같은 파일 두 개가 각 압축 파일에서 공간을 독립적으로 점유합니다. 

그에 반해 이미지는 내용이 같은 레이어1, 레이어2를 공유하기 때문에 전체 공간에서 봤을 때 상대적으로 용량을 적게 차지합니다.

![image](https://user-images.githubusercontent.com/62640332/168470277-c4430b0f-8ccd-47b3-bee8-0a72be448bc1.png)

```
다운로드한 이미지 조회
docker images <이미지 이름>
```

![image](https://user-images.githubusercontent.com/62640332/168470821-27c5a5ea-d75e-4ca8-8e63-50fbcc67a015.png)

stable과 latest 이미지의 크기는 각각 132MB, 133MB이지만, 실제로는 69.2MB에 해당하는 레이어를 두 이미지가 공유하고 있습니다

<br>
<br>

### 2. 컨테이너 실행

---

docker run으로 컨테이너를 생성하면 결괏값으로 16진수 문자열이 나옵니다. 

이 문자열은 컨테이너를 식별할 수 있는 고유한 `ID`

컨테이너를 생성하게 되면 마스터 노드 내부에 존재하게 된다. 

그리고 curl 127.0.0.1 명령으로 웹 페이지 정보를 가져 올려고 시도하면 Connection refuesed 오류가 발생한다.

이유는 로컬 호스트(127.0.0.1)의 지정한 포트로 전달만 될 뿐 컨테이너까지 도달하지 못해서 이다.

호스트에 도달한 후 컨테이너로 도달하기 위한 추가 경로가 설정 필요하다.

![image](https://user-images.githubusercontent.com/62640332/168471382-3a3ea4a9-9395-4c02-9bf2-b2c46559808f.png)

```
컨테이너는 변경 불가능한 인프라(immutable infrastructure)를 지향합니다. 
변경 불가능한 인프라는 초기에 인프라를 구성하면 임의로 디렉터리 연결이나 포트 노출과 같은 설정을 변경할 수 없습니다. 
따라서 컨테이너에 적용된 설정을 변경하려면 새로운 컨테이너를 생성해야 합니다. 
이러한 특성 덕분에 컨테이너로 배포된 인프라는 배포된 상태를 유지한다는 장점이 있습니다.
```

```
- 위의 에러가뜬 컨테이너 실행 방법
[root@m-k8s ~]# docker run -d --restart always nginx

- 추가로 경로를 설정해 정상적으로 컨테이너 실행 방법
[root@m-k8s ~]# docker run -d -p 8080:80 --name nginx-exposed --restart always nginx
```

<br>
<br>

### 3. 디렉토리와 연결하기

---

![image](https://user-images.githubusercontent.com/62640332/168472006-9454454f-c550-402b-b1df-92cb60ced0bf.png)

![image](https://user-images.githubusercontent.com/62640332/168472111-251ee1c2-f855-4df9-9d89-7a2e2996ba01.png)

<br>
<br>

### 4. 컨테이너 삭제 및 이미지 삭제

---

- 볼륨(volume)은 도커가 직접 관리하며 컨테이너에 제공하는 호스트의 공간

- 컨테이너나 이미지를 삭제하기 전에 먼저 컨테이너를 정지해야 합니다. 
  
  삭제할 때 말고도 동일한 호스트의 포트를 사용하는 컨테이너를 배포하거나 작동 중인 컨테이너의 사용 자체를 종료할 때도 먼저 컨테이너를 정지해야 합니다.

```
- 컨테이너 정지 명령
docker stop <컨테이너 이름 | ID>

- 컨테이너 삭제 명령
docker rm <컨테이너 이름 | ID>

- 이미지 삭제 명령
docker rmi <이미지 이름 | ID>
```

<br>
<br>

### ㅁ 컨테이너 이미지 만들기

---

컨테이너 이미지 만드는 방법

1. 기본적인 빌드
2. 용량 줄이기
3. 컨테이너 내부 빌드
4. 멀티 스테이지


#### 1. 기본 방법으로 빌드하기

컨테이너 빌드 과정

![image](https://user-images.githubusercontent.com/62640332/168473881-fd271f0b-6218-4ab9-b42c-a86208113619.png)

```
컨테이너 이미지 빌드 명령어
docker build <생성할 파일이름> <생성할 파일위치>
```

- 목적지에 따라 출발지 표시가 다른 이유
    

현재 구동 중인 호스트의 가상 인터페이스 IP(192.168.1.10)와 로컬호스트 IP(127.0.0.1)의 60431번 포트에 요청을 보내면 출발지의 IP가 다름을 확인할 수 있습니다.

```
[root@m-k8s 4.3.1]# curl 192.168.1.10:60431
src: 192.168.1.10 / dest: 192.168.1.10
[root@m-k8s 4.3.1]# curl 127.0.0.1:60431
src: 172.17.0.1 / dest: 127.0.0.1
```

현재 컨테이너는 외부 요청이 목적지에 도착하기 전에 거친 네트워크 인터페이스의 IP와 포트를 출발지(src)로 표시하게 작성됐습니다. 

그런데 호스트 인터페이스(eth1)의 IP가 192.168.1.10이고, 컨테이너 브리지 인터페이스(docker0)의 IP는 172.17.0.1입니다. 

eth1은 외부 요청을 받아들이는 네트워크 인터페이스이고, docker0는 도커 컨테이너가 사용하는 네트워크 인터페이스입니다. 

따라서 도커 컨테이너가 외부와 통신하려면 docker0를 거쳐야 합니다.

![image](https://user-images.githubusercontent.com/62640332/168474256-b9b686c7-0f23-4295-85d6-53f8342f7500.png)


도커 컨테이너는 생성될 때 docker0에 부여된 172.17.0.0/16 범위에 해당하는 172.17.0.2~172.17.255.254(172.17.0.1은 docker0에서 사용) 사이의 IP를 할당받습니다. 

따라서 현재 생성된 컨테이너는 172.17.0.0/16에 속하는 IP를 가진 상태입니다. 

이 컨테이너는 모든 네트워크 어댑터(0.0.0.0)의 60431번 포트로 들어오는 요청을 컨테이너 내부로 전달하도록 옵션으로 설정했습니다. 

이 옵션을 설정하면 외부 IP에서 들어오는 요청을 컨테이너 내부 IP로 전달하는 경로 전달 규칙이 리눅스 호스트에 설정됩니다. 

그 결과 컨테이너 내부로 요청을 보낼 경우 출발지는 앞에서 실행한 첫 번째 명령의 수행 결과와 같이 192.168.1.10:60431이 표시됩니다.

반면에 127.0.0.1이나 localhost와 같은 내부 IP에서 컨테이너 내부로 요청을 전달할 경우 docker-proxy 프로세스가 컨테이너 내부로 요청을 전달하는 역할을 하며 

이 과정에서 도커가 사용하는 네트워크 인터페이스의 IP인 172.17.0.1을 출발지로 사용합니다. 

따라서 docker-proxy 프로세스에 문제가 발생하면 내부 IP로는 컨테이너 내부에 요청을 전달할 수 없을 뿐만 아니라 컨테이너 내부에서 호스트 외부 IP와 포트로 요청을 보낼 수 없는 문제가 발생합니다. 

자세한 내용은 이 책의 범위를 넘어가므로 더 설명하지 않습니다. 궁금하신 분은 인터넷에서 헤어핀(Hairpin) NAT에 관해 찾아보기 바랍니다.

<br>
<br>

#### 2. 콘테이너 용량 줄이기

![image](https://user-images.githubusercontent.com/62640332/168474616-59babac5-1876-4a1f-809b-c7bc9f876b22.png)

openjdk 사용 안하고 GCR에서 제공하는 distroless로 변경

<br>

#### 3. 컨테이너 내부에서 컨테이너 빌드하기

![image](https://user-images.githubusercontent.com/62640332/168478972-4fb07073-93c6-4ddd-8360-8a93adc2641a.png)

빌드 과정 자체를 openjdk 이미지에서 진행하므로 Dockerfile만 필요

새로 생성된 nohost-img가 618MB로 이미지 중에서 가장 용량이 큽니다. 

nohost-img는 컨테이너 내부에서 빌드를 진행하기 때문에 빌드 중간에 생성한 파일들과 내려받은 라이브러리 캐시들이 최종 이미지인 nohost-img에 그대로 남습니다. 

따라서 빌드 최종 결과물만 전달했던 basic-img보다 더 커지게 됩니다.

컨테이너 이미지는 커지면 커질수록 비효율적으로 작동할 수밖에 없습니다. 따라서 openjdk로 컨테이너 내부에서 컨테이너를 빌드하는 것을 좋지 않은 방법입니다.

<br>

#### 4. 최적화해 컨테이너 빌드하기

멀티 스테이지 빌드(Multi-Stage Build, 이후 멀티 스테이지라고 표현함) 방법은 최종 이미지의 용량을 줄일 수 있고 호스트에 어떠한 빌드 도구도 설치할 필요가 없습니다.

![image](https://user-images.githubusercontent.com/62640332/168479244-d968508b-3dc0-4b44-b17c-1c27b2099471.png)

멀티 스테이지의 핵심은 빌드하는 위치와 최종 이미지를 `분리`하는 것입니다. 

그래서 최종 이미지는 빌드된 JAR을 가지고 있지만, 용량은 줄일 수 있습니다.

<br>
<br>

### ㅁ 쿠버네티스에서 직접 만든 컨테이너 사용하기

---

#### 1. 쿠버네티스에서 도커 이미지 구동하기

: 쿠버네티스는 컨테이너를 효과적으로 다루기 위해 만들어짐, 컨테이너인 파드도 쉽게 부를수 있습니다.

따라서 직접 만든 컨테이너 이미지도 kubectl 명령으로 쿠버네티스 클러스터에서 바로 구동 가능

#### 2. 레지스트리 구성하기

: 호스트에서 생성한 이미지를 쿠버네티스에서 사용하려면 모든 노드에서 공통으로 접근하는 레지스트리(저장소)가 필요합니다.

도커나 쿠버네티스는 `도커 허브`라는 레지스트리에서 이미지 다운로드 가능.

그러나 무료로 사용하기에는 제약이 많기에 직접 구축하여 저장소 만들어야함.

도커에서 제공하는 `도커 레지스트리`(Docker Registry)이미지를 사용해 사설 도커 레지스트리 만드는 테스트

도커 레지스트리는 기능은 부족하나, 컨테이너를 하나만 구동하면 돼서 설치가 간편하고 내부에서 테스트 목적으로 사용하기 적합

다음 아래는 도커 레지스트리 외에 사용할 수 있는 레지스트리

- Quay(키): 레드햇에서 제공하는 이미지 레지스트리입니다. 오픈 소스로 제공되는 무료 버전과 구입한 후 보유한 서버에 직접 설치해 사용할 수 있는 유료 버전, 비용을 지불하고 클라우드에서 이용할 수 있는 서비스형 상품이 있습니다. 유료 버전이나 서비스형 상품은 제품에 대한 신뢰성 보증과 기술 지원 서비스를 받을 수 있으므로 안정적인 서비스를 운영하고 싶은 사용자에게 적합합니다.


- Harbor(하버): 클라우드 네이티브 컴퓨팅 재단의 지원을 받는 Project Harbor에서 오픈 소스로 제공하는 레지스트리입니다. 도커 이미지 외에도 헬름 차트도 저장할 수 있으며 이에 대해서는 5.2.3절에서 살펴보겠습니다. 이미지와 헬름 차트를 함께 저장할 수 있어 두 가지 모두를 사용하는 사용자에게 알맞습니다.

- Nexus Repository(넥서스 리포지터리): Sonatype에서 만든 레지스트리로, 오픈 소스로 제공되는 무료 버전과 유료 버전이 있습니다. 유료 버전은 기술 지원과 다양한 기능을 제공받을 수 있습니다. 도커 이미지 외에도 리눅스 설치 패키지, 자바 라이브러리, 파이썬 라이브러리 등 다양한 형식의 파일을 저장할 수 있어서 여러 형식의 패키지를 하나의 저장소에 관리하려는 사용자에게 안성맞춤입니다. 다양한 형식을 지원한다는 매력적인 특성 덕분에 레지스트리 중 가장 많은 사용자를 보유하고 있습니다.

- Nexus Repository(넥서스 리포지터리): Sonatype에서 만든 레지스트리로, 오픈 소스로 제공되는 무료 버전과 유료 버전이 있습니다. 유료 버전은 기술 지원과 다양한 기능을 제공받을 수 있습니다. 도커 이미지 외에도 리눅스 설치 패키지, 자바 라이브러리, 파이썬 라이브러리 등 다양한 형식의 파일을 저장할 수 있어서 여러 형식의 패키지를 하나의 저장소에 관리하려는 사용자에게 안성맞춤입니다. 다양한 형식을 지원한다는 매력적인 특성 덕분에 레지스트리 중 가장 많은 사용자를 보유하고 있습니다.

![image](https://user-images.githubusercontent.com/62640332/210388030-c818593e-ed0a-4e07-82fc-0a0cb760ca08.png)

<br>
<br>

컨테이너 인프라 환경에서 쿠버네티스를 사용하는 이유는 컨테이너 애플리케이션을 유연하고 빠르게 배포하고 운영 위해서 입니다.

컨테이너로 구동하는 애플리케이션을 어떻게 배포하는것이 가장 좋은가?

`파이프 라인`(PipeLine), 아래의 과정은 지금까지 도커 배포 workflow 이며, 이것을 파이프 라인 이라고 합니다.

1. docker build - 깃허브 등의 저장소에 저장해 둔 애플리케이션 소스 코드를 내려받아 도커 컨테이너 이미지로 빌드합니다.

2. docker push - 빌드한 컨테이너 이미지를 쿠버네티스에서 사용할 수 있도록 레지스트리에 등록합니다.

3. kubectl create - 레지스트리에 등록된 이미지를 기반으로 쿠버네티스 오브젝트를 생성합니다.

4. kubectl expose - 생성한 오브젝트(파드/디플로이먼트)를 외부에서 접속할 수 있도록 서비스 형태로 노출합니다.

<br>
<br>

### ㅁ 컨테이너 인프라 환경에서 CI/CD

---

CI는 코드를 커밋하고 빌드했을 때 정상적으로 작동하는지 반복적으로 검증해 애플리케이션의 신뢰성을 높이는 작업입니다. 

CI 과정을 마친 애플리케이션은 신뢰할 수 있는 상태가 됩니다. 

CD는 CI 과정에서 생성된 신뢰할 수 있는 애플리케이션을 실제 상용 환경에 자동으로 배포하는 것을 의미합니다.

애플리케이션을 상용 환경에 배포할 때 고려해야 할 사항이 여러 가지 있는데, 이를 CD에 미리 정의하면 실수를 줄이고, 실제 적용 시간도 최소화할 수 있습니다.

![image](https://user-images.githubusercontent.com/62640332/210392203-500f87e0-f97f-4071-93e4-a05fa228a5e1.png)

CI/CD를 컨테이너 인프라 관점에서 정리해 보겠습니다. 

개발자가 소스를 커밋(Commit)하고 푸시(Push)하면 CI 단계로 들어갑니다. 

CI 단계에서는 애플리케이션이 자동 빌드되고 테스트를 거쳐 배포할 수 있는 애플리케이션인지 확인합니다. 

테스트를 통과하면 신뢰할 수 있는 애플리케이션으로 간주하고 CD 단계로 넘어갑니다. 

CD 단계에서는 애플리케이션을 컨테이너 이미지로 만들어서 파드, 디플로이먼트, 스테이트풀셋 등 다양한 오브젝트 조건에 맞춰 미리 설정한 파일을 통해 배포합니다.

### ㅁ CI/CD 도구 비교

---

- 팀시티(Teamcity): 젯브레인즈(Jetbrains)에서 만든 CI/CD 도구로, 코틀린(Kotlin)을 기반으로 만든 Kotlin DSL이라는 스크립트 언어로 작업을 구성할 수 있습니다. 빌드 작업을 수행하는 에이전트 3개와 빌드 작업 100개를 무료로 사용할 수 있고, 더 많은 에이전트를 사용하려면 유료 결제를 해야 합니다.

-  깃허브 액션(Github Action): 깃허브에서 지원하는 워크플로(Workflow) 기반의 CI/CD 도구입니다. 깃허브에 저장한 소스 코드를 자동 분석한 결과를 기반으로 깃허브 액션이 추천하는 방식에 따라 워크플로를 구성하거나 사용자가 직접 워크플로를 정의하는 파일을 작성한 후 깃허브 저장소에 넣어 사용할 수 있습니다. 깃허브 저장소에 소스 코드를 공개할 경우 깃허브 액션을 무료로 사용할 수 있으나 한 달에 2,000분이라는 제한 시간이 있습니다. 추가로 사용할 경우에는 분 단위의 요금이 별도로 부과됩니다.

-  뱀부(Bamboo): 아틀라시안(Atlassian)에서 만든 CI/CD 도구로, 유료이며 사용자의 서버에 설치해 사용합니다. 뱀부는 아틀라시안에서 만든 다른 협업 도구를 사용 중이라면 연계해 사용하기 좋습니다.

-  젠킨스(Jenkins): 오픈 소스 CI/CD 도구로, 2004년 출시 당시에는 허드슨(Hudson)이라는 이름을 사용했으나 현재는 젠킨스로 이름이 바뀌었습니다. 젠킨스는 사용자가 직접 UI에서 작업을 구성하거나 작업 순서를 코드로 정의할 수 있습니다. 역사, 인지도, 사용자 수에서 CI/CD 도구의 대명사라고 해도 무리가 없을 정도로 널리 알려져 있습니다. 오랜 시간 동안 많은 사람이 사용하고 있어서 사용에 필요한 정보를 찾기 쉽고 활용 방법과 플러그인 개발 관련 커뮤니티 활동이 활발해서 다양한 사용 환경, 언어 및 빌드 도구와 연계할 플러그인이 필요할 경우 인터넷에서 대부분의 플러그인을 쉽게 찾을 수 있습니다. 특정 언어나 환경에 구애받지 않고 범용적인 목적으로 무난하게 쓸 수 있습니다.

![image](https://user-images.githubusercontent.com/62640332/210393478-973e617e-bc52-44e5-9c51-8637e05793eb.png)

앞의 도구 외에도 퍼블릭 클라우드 기반의 시스템일 때는 클라우드 서비스 제공 업체에서 배포하는 CI/CD 도구(AWS CodeBuild, CodePipeline, CodeDeploy, GCP CloudBuild, Azure Pipelines)를, 배포가 중요한 환경에서는 CD 기능에 중점을 둔 스핀네이커(Spinnaker)나 아르고 CD(Argo CD)를 선택적으로 도입할 수도 있습니다.


### ㅁ 배포 간편화 도구 비교

---

kubectl은 사실 바이너리 실행 파일로 짜인 배포 도구입니다. 

만약 kubectl이 없다면 직접 코드를 짜서 API 서버에 명령을 내려야 합니다. 

커스터마이즈와 헬름은 kubectl을 좀 더 확장해서 복잡한 오브젝트와 구성 환경을 자동으로 맞추는 도구입니다

• 큐브시티엘(kubectl): 쿠버네티스에 기본으로 포함된 커맨드라인 도구로, 추가 설치 없이 바로 사용할 수 있습니다. 오브젝트 생성과 쿠버네티스 클러스터에 존재하는 오브젝트, 이벤트 등의 정보를 확인하는 데 사용하는 활용도 높은 도구입니다. 또한 오브젝트의 명세가 정의된 야믈 파일을 인자로 입력받아 파일 내용에 따라 오브젝트를 배포할 수도 있습니다. 큐브시티엘은 정의된 매니페스트 파일을 그대로 배포하기 때문에 개별적인 오브젝트를 관리하거나 배포할 때 사용하는 것이 좋습니다.

• 커스터마이즈(kustomize): 오브젝트를 사용자의 의도에 따라 유동적으로 배포할 수 있습니다. 별도의 커스터마이즈 실행 파일을 활용해 커스터마이즈 명세를 따르는 야믈 파일을 생성할 수 있습니다. 야믈 파일이 이미 존재한다면 kubectl로도 배포할 수 있는 옵션(-k)이 있을 정도로 kubectl과 매우 밀접하게 동작합니다. 커스터마이즈는 명세와 관련된 야믈 파일에 변수나 템플릿을 사용하지는 않지만, 명령어로 배포 대상 오브젝트의 이미지 태그와 레이블 같은 명세를 변경하거나 일반 파일을 이용해 컨피그맵과 시크릿을 생성하는 기능을 지원합니다. 그래서 운영 중인 환경에서 배포 시 가변적인 요소를 적용하는 데 적합합니다.

• 헬름(Helm): 헬름은 쿠버네티스 사용자의 70% 이상이 사용하고 있을 정도로 널리 알려진 도구로, 오브젝트 배포에 필요한 사양이 이미 정의된 차트(Chart)라는 패키지를 활용합니다. 앞선 두 가지 도구와 달리 헬름 차트 저장소가 온라인에 있기 때문에 패키지를 검색하고 내려받아 사용하기가 매우 간편합니다. 헬름 차트는 자체적인 템플릿 문법을 사용하므로 가변적인 인자를 배포할 때 적용해 다양한 배포 환경에 맞추거나 원하는 조건을 적용할 수 있습니다. 헬름은 오브젝트를 묶어 패키지 단위로 관리하므로 단순한 1개의 명령어로 애플리케이션에 필요한 오브젝트들을 구성할 수 있습니다.

![image](https://user-images.githubusercontent.com/62640332/210396676-860600e4-2ce6-4028-a6c4-66137743a79a.png)


<br>
<br>

### ㅁ 커스터마이즈 배포 간편화

---

커스터마이즈는 야믈 파일에 정의된 값을 사용자가 원하는 값으로 변경할 수 있습니다. 

쿠버네티스에서 오브젝트에 대한 수정 사항을 반영하려면 사용자가 직접 야믈 파일을 편집기 프로그램으로 수정해야 합니다.

일반적으로 이런 방식으로 수정했을 때 큰 문제가 발생하지 않습니다. 

그런데 만약 수정해야 하는 야믈 파일이 매우 많거나 하나의 야믈 파일로 환경이 다른 여러 개의 쿠버네티스 클러스터에 배포해야 해서

LABEL이나 NAME 같은 일부 항목을 수정해야 한다면 매번 일일이 고치는 데 많은 노력이 듭니다. 

커스터마이즈는 이를 위해 `kustomize` 명령을 제공합니다. 

kustomize 명령과 create 옵션으로 kustomization.yaml이라는 기본 매니페스트를 만들고, 이 파일에 변경해야 하는 값들을 적용합니다. 

그리고 build 옵션으로 변경할 내용이 적용된 최종 야믈 파일을 저장하거나 변경된 내용이 바로 실행되도록 지정합니다.

커스터마이즈를 사용해서 MetalLB를 만든다는 것은 사실상 명세서인 kustomization.yaml을 만드는 과정입니다. 

그리고 만들어진 kustomization.yaml을 통해서 우리가 원하는 내용이 담겨 있는 매니페스트를 생성하고, 이 매니페스트를 통해서 배포하는 것입니다. 

즉 커스터마이즈는 단순히 `최종 매니페스트 생성을 도와주는 도구`인 것입니다.

커스터마이즈를 이용하면 MetalLB의 다양한 설정을 사용자의 입맛에 맞게 변경하고 구현할 수 있습니다. 

그러나 커스터마이즈는 여러 가지 변경할 부분을 사용자가 직접 kustomization.yaml에 추가하고 최종적으로 필요한 매니페스트를 만들어 배포해야 합니다. 

이러한 다소 수동적인 작성 방식이 아닌 선언적으로 필요한 내용을 제공하고 이에 맞게 바로 배포하려면 어떻게 해야 할까요? 

그리고 커스터마이즈를 통해서 변경할 수 없었던 주소 할당 영역과 같은 값도 배포 시에 같이 변경하려면 어떻게 할까요?

`헬름`은 이러한 제약 사항들을 없애고 편리성을 높일 수 있습니다.

### ㅁ 헬름 배포 간편화

헬름은 쿠버네티스에 패키지를 손쉽게 배포할 수 있도록 패키지를 관리하는 쿠버네티스 전용 패키지 매니저입니다. 

일반적으로 패키지는 실행 파일뿐만 아니라 실행 환경에 필요한 의존성 파일 과 환경 정보들의 묶음입니다. 

그리고 패키지 매니저는 외부에 있는 저장소에서 패키지 정보를 받아와 패키지를 안정적으로 관리하는 도구입니다. 

패키지 매니저는 다양한 목적으로 사용되지만, 가장 중요한 목적은 설치에 필요한 의존성 파일들을 관리하고 간편하게 설치할 수 있도록 도와주는 것입니다. 

플랫폼별 패키지 매니저의 저장소 위치와 사용 목적은 다음과 같습니다.

![image](https://user-images.githubusercontent.com/62640332/210591684-dba9d76a-d977-4148-a87d-3d9c35c7f155.png)

- 패키지 매니저 기능

1. 패키지 검색: 설정한 저장소에서 패키지를 검색하는 기능을 제공합니다. 이때 대부분 저장소는 목적에 따라 변경할 수 있습니다.

2. 패키지 관리: 저장소에서 패키지 정보를 확인하고, 사용자 시스템에 패키지 설치, 삭제, 업그레이드, 되돌리기 등을 할 수 있습니다.

3. 패키지 의존성 관리: 패키지를 설치할 때 의존하는 소프트웨어를 같이 설치하고, 삭제할 때 같이 삭제할 수 있습니다.

4. 패키지 보안 관리: 디지털 인증서와 패키지에 고유하게 발행되는 체크섬(Checksum)이라는 값으로 해당 패키지의 소프트웨어나 의존성이 변조됐는지 검사할 수 있습니다.


컨테이너 인프라 환경에서 애플리케이션을 배포하려면 ConfigMap, ServiceAccount, PV, PVC, Secret 등 애플리케이션 배포 구성에 필요한 모든 쿠버네티스 오브젝트를 작성하고, 

kubectl 명령을 실행해서 쿠버네티스 클러스터에 설치해야 합니다. 

이때 커스터마이즈를 사용하면 많은 부분을 환경에 맞춰 변경할 수 있지만, 주소 할당 영역과 같은 정보는 값의 형태가 아니라서 변경할 수가 없습니다. 

이런 경우에 헬름을 사용하면 주소 할당 영역도 변경이 가능합니다.
 
커스터마이즈에서 변경할 수 없는 값을 환경에 맞게 변경할 수 있다는 점 외에도 헬름은 여러 장점이 있습니다.

다수의 오브젝트 배포 야믈은 파일 구분자인 '---'로 묶어 단일 야믈로 작성해 배포할 수 있습니다. 

이런 경우 변경 사항을 추적할 때 모든 내용이 한 야믈 파일에 담겨 있기 때문에 여러 사람이 동시에 작업하면 충돌(conflict)이 발생할 수 있습니다. 

문제를 해결하려면 목적에 맞게 디렉터리를 만들고 야믈 파일을 분리해 관리하면서 배포 시에는 디렉터리를 kubectl apply -f의 인자로 넘겨줘야 합니다. 

하지만 이런 방식을 사용하면 요구 조건에 변경되는 야믈 파일을 매번 개별 디렉터리에 작성해야 하고 디렉터리가 늘어날수록 관리 영역도 늘어나게 됩니다.

이럴 때 `헬름`을 사용하면 요구 조건별로 리소스를 편집하거나 변수를 넘겨서 처리하는 `패키지`를 만들 수 있습니다. 

이렇게 다양한 요구 조건을 처리할 수 있는 패키지를 `차트(chart)`라고 하는데, 이를 헬름 저장소에 공개해 여러 사용자와 공유합니다. 

각 사용자는 공개된 저장소에 등록된 차트를 이용해서 애플리케이션을 원하는 형태로 쿠버네티스에 배포할 수 있습니다.

또한, 헬름은 배포한 애플리케이션을 업그레이드하거나 되돌릴 수 있는 기능과 삭제할 수 있는 기능을 제공합니다.

이처럼 헬름을 이용하면 하나의 패키지로 다양한 사용자가 원하는 각자의 환경을 구성할 수 있으며 이를 자유롭게 배포, 관리, 삭제할 수 있습니다.

헬름 기본 저장소는 아티팩트허브(artifacthub.io)로, 다른 패키지 매니저처럼 외부에 있습니다. 다른 저장소와 달리 아티팩트허브에서는 설치할 패키지에 대한 경로만을 제공합니다.

<br>
<br>

### ㅁ 헬름 작동 과정

---

![image](https://user-images.githubusercontent.com/62640332/210592818-15c0072e-a149-416b-a78c-32faf3eb844f.png)

• 생산자 영역: 생산자가 헬름 명령으로 작업 공간을 생성하면 templates 디렉터리로 애플리케이션 배포에 필요한 여러 야믈 파일과 구성 파일을 작성할 수 있습니다. 

이때 templates 디렉터리에서 조건별 분기, 값 전달 등을 처리할 수 있도록 values.yaml에 설정된 `키`를 사용합니다. 

이때 값이 전달되지 않으면 기본값으로 처리하도록 values.yaml에 설정할 수 있습니다. 

이렇게 필요한 패키지의 여러 분기 처리나 배포에 대한 구성이 완료되면 생산자는 차트의 이름, 목적, 배포되는 애플리케이션 버전과 같은 패키지 정보를 Charts.yaml에 채워 넣습니다. 

앞의 과정을 모두 거쳐 차트 구성이 완료되면 생산자가 생산자 저장소에 업로드합니다. 

그리고 업로드한 생산자 저장소를 아티팩트허브에 등록하면 사용자는 아티팩트허브에서 생산자가 만든 저장소를 찾을 수 있습니다.

<br>

• 아티팩트허브 영역: 아티팩트허브 검색을 통해 사용자가 찾고자 하는 애플리케이션 패키지를 검색하면 해당 패키지가 저장된 주소를 확인합니다. 

이렇게 확인한 주소는 각 애플리케이션을 개발하는 주체가 관리합니다. 

헬름 버전2에서는 이러한 차트 저장소를 개인이 아닌 CNCF가 관리했으나 헬름 버전3가 배포되고 나서는 CNCF가 관리해야 하는 차트가 많아지면서 모든 차트 저장소는 개인 및 단체에서 직접 관리하도록 정책을 변경했습니다. 

참고로 CNCF가 직접 관리하던 헬름 버전2의 차트 저장소는 하위 호환성을 위해서 내려받기 기능만 제공하고 있습니다.

<br>

• 사용자 영역: 사용자는 설치하려는 애플리케이션의 차트 저장소 주소를 아티팩트허브에서 얻으면 헬름을 통해서 주소를 등록합니다. 

그리고 이를 최신으로 업데이트한 이후에 차트를 내려받고 설치합니다. 

이렇게 헬름을 통해 쿠버네티스에 설치된 애플리케이션 패키지를 `릴리스(Release)`라고 합니다. 

헬름을 통해 배포된 릴리스를 다시 차트를 사용해 업그레이드할 수 있고 원래대로 되돌릴 수 있습니다. 또한, 사용하지 않는 헬름 릴리스를 제거할 수도 있습니다.

<br>

```
# 헬름을 이미 알고 있다면 설치 과정이 조금 다르다고 생각할 수 있습니다. 

이 책에서 설치하는 헬름은 버전3인데, 헬름 버전3에서는 헬름 버전2보다 간결한 구조로 내부 구성을 변경해 기존의 구성 요소가 다수 사라졌습니다.

이전까지 널리 쓰이던 헬름 버전2는 헬름 클라이언트를 설치하고 클라이언트가 쿠버네티스 API 서버와 통신하면서 오브젝트를 관리하는 방식이었고, 이를 위해 틸러(Tiller)라고 하는 서버를 설치했습니다. 

이외에도 쿠버네티스와 틸러가 통신하기 위한 권한 설정 등을 해야 했습니다. 

헬름 버전3은 2019년 11월 출시됐으며 확산되는 단계입니다. 그래서 헬름을 검색했을 때 헬름의 아키텍처에서 틸러를 설명하는 자료를 종종 찾아볼 수 있지만, 헬름 버전3에서는 사라진 구성 요소입니다.
```
<br>
<br>

### ㅁ 모니터링 도구(데이터 수집과 통합도구)

---

• 프로메테우스(Prometheus): 사운드클라우드(SoundCloud)에서 자사 서비스의 모니터링을 위해 개발한 도구입니다. 

현재는 오픈 소스로 전환돼 CNCF에서 관리하며, 2018년 8월에 졸업(Graduated) 프로젝트가 됐습니다. 

시계열 데이터베이스를 내장하고 있고, 자체적인 질의 페이지 외에 시각화 기능은 없으나 그라파나와 연계하면 현업에서 사용할 수 있는 시각화 기능을 제공할 수 있습니다. 

중앙 서버에서 에이전트의 데이터를 수집하는 풀(Pull) 방식을 사용하므로 쿠버네티스 환경에서 설치된 에이전트를 통해 노드와 컨테이너 상태를 모두 수집해 모니터링할 수 있습니다. 

그리고 에이전트를 통해 내부 메트릭을 외부로 노출하기 때문에 사용자가 수집 대상에 접속할 수만 있다면 개인 컴퓨터에서도 메트릭을 가져올 수 있습니다. 

따라서 모니터링과 관련된 개발을 하기에 편합니다. 또한 일회성으로 모니터링 대상에 대한 세부적인 메트릭도 간단하게 웹 브라우저로 확인할 수 있습니다. 

프로메테우스는 완전한 오픈 소스 모델을 선택해 사용자 층이 넓고 관련 자료가 많으며 각종 대시보드 도구나 메신저 등이 프로메테우스와의 연계를 지원하므로 직접 모니터링 시스템을 구축할 때 좋습니다.

<br>

• 데이터독(Datadog): 모니터링 데이터를 업체에서 관리하는 경우를 서비스형이라고 하고, 데이터를 사용자가 직접 관리하는 경우를 설치형이라고 하는데, 데이터독은 서비스형 소프트웨어(SaaS) 형태로 제공합니다. 

그래서 웹사이트에서 모니터링 대상을 관리할 수 있고 쿠버네티스를 비롯해 여러 클라우드 서비스나 애플리케이션과 연결이 쉬우므로 관리 부담이 적다는 장점이 있습니다. 

그러나 모니터링 대상마다 요금을 부과하기 때문에 모니터링 대상이 늘면 비용이 커지는 단점이 있습니다.

<br>

• 뉴 렐릭(New Relic): 데이터독과 같은 서비스형 소프트웨어입니다. 

다만 데이터독과 비교했을 때 애플리케이션 성능 모니터링(APM, Application Performance Monitoring)에 더 특화돼 있습니다. 

데이터독과 마찬가지로 모니터링 대상이 많을수록 비용이 커집니다.

<br>

• 인플럭스DB(InfluxDB): 2013년 인플럭스데이터(Influxdata)에서 개발한 시계열(Time series) 데이터베이스입니다. 

오픈 소스로 공개된 무료 버전과 클라우드에서 바로 사용할 수 있는 서비스형 소프트웨어, 라이선스를 구매해 설치할 수 있는 엔터프라이즈 버전, 총 3가지 유형이 있습니다. 

무료 버전은 프로메테우스와 유사하지만 인플럭스DB가 쓰기 성능이 더 뛰어나 대량의 이벤트를 모니터링하는 데는 좀 더 적합합니다. 

엔터프라이즈 버전은 프로메테우스에서 부족한 부분인 고가용성을 위한 분산 저장을 좀 더 쉽게 구성할 수 있는 기능을 제공합니다. 

인플럭스DB는 프로메테우스와 더불어 오픈 소스로 모니터링 플랫폼을 구축하기 위한 최선의 도구이고, 유료 서비스라는 선택지까지 있어 선택의 폭이 조금 더 넓습니다. 

하지만 간단한 구성으로 데이터를 받아오는 프로메테우스보다 상대적으로 구성이 어렵다는 단점이 있습니다.

![image](https://user-images.githubusercontent.com/62640332/210601220-3fcb13ba-4e7a-4f9a-9fe6-3b3bf19d7bd6.png)

### ㅁ 모니터링 도구(데이터 수집과 통합도구)

---

• 그라파나(Grafana): 그라파나 랩스(Grafana Labs)에서 개발했으며, 특정 소프트웨어에 종속되지 않은 독립적인 시각화 도구입니다. 30가지 이상의 다양한 수집 도구 및 데이터베이스들과 연계를 지원합니다. 

주로 시계열 데이터의 시각화에 많이 쓰지만, 관계형 데이터베이스 데이터를 표 형태로 시각화해 사용할 수도 있습니다. 

그라파나는 기능을 확장하는 플러그인과 개별 사용자들이 만들어 둔 대시보드의 공유가 매우 활발해 필요에 따라 적절히 선택해 사용하면 됩니다. 

그라파나는 오픈 소스라서 사용자의 요구 사항에 맞게 수정이 가능하고, 필요에 따라 설치형과 서비스형을 모두 선택할 수 있습니다.

<br>

• 키바나(Kibana): 엘라스틱서치(ElasticSearch)를 개발한 엘라스틱에서 만든 시각화 도구입니다. 

엘라스틱서치에 적재된 데이터를 시각화하거나 검색하는 데 사용하고, 이러한 데이터를 분석할 때도 사용합니다.

엘라스틱서치의 데이터만을 시각화할 수 있기 때문에 프로메테우스의 시계열 데이터를 메트릭비트(Metricbeat)라는 도구로 엘라스틱서치에 전달해야 하는 불편함이 있습니다. 

하지만 시각화 기능이 매우 강력해서 시각화를 중점적으로 다루는 경우에 사용하면 좋습니다.

<br>

• 크로노그래프(Chronograf): 인플럭스DB를 개발한 인플럭스데이터에서 만든 시각화 도구로, 오픈 소스로 제공돼 사용자의 편의에 맞게 수정할 수 있습니다. 

설치형과 서비스형을 모두 제공해 용도에 따라 선택하면 됩니다. 

키바나와 마찬가지로 자사 제품인 인플럭스DB만 시각화할 수 있으므로 다양한 대상을 시각화할 수 없습니다.

<br>

![image](https://user-images.githubusercontent.com/62640332/210601859-015475d8-3bcc-43fe-8d85-2ccc90774456.png)

<br>
<br>

###  Prometheus

---

-  메트릭값의 종류

1. 카운터(Counter): 누적된 값을 표현하는 데 사용하는 메트릭 타입입니다. 

카운터에 누적된 값으로 구간별로 변화율을 파악해 해당 값이 어느 정도로 추세로 증가하는지 알 수 있습니다. 

그래서 이벤트나 오류 등이 급증하는 구간을 파악하는 데 적합합니다. 

값이 누적되기 때문에 특정 순간의 데이터를 표현하는 데는 적합하지 않습니다. 

카운터는 값을 중점적으로 보기보다 값이 얼마만큼 변했는지 변화율을 주로 확인합니다.

<br>

2. 게이지(Gauge): 특정 시점의 값을 표현하는 데 사용하는 메트릭 타입입니다. 

카운터가 누적된 값을 표현해 증가만을 고려하는 것과 달리 게이지는 시점별로 증가나 감소를 모두 표현할 수 잇습니다. 

CPU 온도나 메모리 사용량 등은 누적된 값이 필요하지 않고 조회하는 순간의 값이 중요하므로 게이지 타입을 사용합니다

<br>

3. 히스토그램(Histogram): 사전에 미리 정의한 구간 안에 있는 메트릭 값의 빈도를 측정합니다. 

이때 익스포터를 구현하는 단계에서 정의한 구간을 버킷(bucket)이라고 합니다. 

예를 들어 히스토그램을 사용해 클라이언트가 서버로 HTTP 요청을 한 경우에 응답 시간과 맞는 버킷에 값을 추가하고 이벤트 횟수를 저장해 표시할 수 있습니다. 

히스토그램은 ‘6.3.5 서머리와 히스토그램’에서 예제와 함께 좀 더 자세히 알아보겠습니다.

<br>

4. 서머리(Summary): 히스토그램과 비슷하게 구간 내에 있는 메트릭 값의 빈도를 측정합니다. 

예를 들어 클라이언트 요청에 따른 응답 시간을 관측하고 저장할 때 사용할 수 있습니다. 

하지만 히스토그램과 달리 구간이 지정되는 것이 아니라 프로메테우스 자체으로 0~1 사이로 구간을 미리 정해 놓습니다. 

서머리도 ‘6.3.5 서머리와 히스토그램’에서 살펴보겠습니다.

<br>

일반적으로 메트릭 이름이 어떤 단어로 끝나느냐에 따라 메트릭 값의 타입을 추정할 수 있습니다. 

total로 끝나면 누적한 값이므로 카운터 타입,

bytes 또는 created라는 단어로 끝나면 해당 시점의 용량 또는 생성됨을 의미하므로 게이지 타입입니다.

<br>

- 메트릭 레이블

: 모든 메트릭 데이터는 하나 이상의 레이블을 가집니다. 

프로메테우스의 레이블은 일반적인 주석이 아니라 메트릭 데이터의 다양한 내용을 표현하는 유일한 방법입니다. 

따라서 단순히 1~2개의 레이블이 아니라 제공하고 싶은 다수의 내용을 key-value 형태로 넣습니다.

<br>

- 메트릭 매처

: 메트릭 레이블에 조건을 줘서 검색하는 방법을 레이블 매처(Label Matchers)라고 합니다. 레이블 매처는 레이블이 존재하면 그에 해당하는 메트릭 데이터를 추출합니다. 레이블 매처에는 앞에서 사용한 '='를 포함해 총 4가지 조건 기호가 있습니다.


• =: 조건에 넣은 값과 레이블 값이 같은 메트릭을 보여줍니다. 예를 들어 {instance="m-k8s"}는 instance 레이블 값이 m-k8s인 메트릭을 찾아 출력합니다.

• !=: 조건에 넣은 값과 레이블 값이 다른 메트릭을 보여줍니다. 예를 들어 {instance!="m-k8s"}는 instance 레이블 값이 m-k8s가 아닌 메트릭을 찾아 출력합니다.

• =~: 조건에 넣은 정규 표현식에 해당하는 메트릭을 보여줍니다. 예를 들어 {instance=~ "w.+"}는 instance 레이블 값이 w로 시작하는 메트릭을 찾아 출력합니다.

• !~: 조건에 넣은 정규 표현식에 해당하지 않는 메트릭을 보여줍니다. 예를 들어 {instance!~ "w.+"}는 instance 레이블 값이 w로 시작하지 않는 모든 메트릭을 찾아 출력합니다.


```
Tip ☆ PromQL 정규 표현식
    
특정한 문자열의 집합을 표현하는 패턴을 정규 표현식이라고 하며, 복잡한 규칙성이 있는 문자열을 검사하는 데 사용합니다. 
정규 표현식의 문법은 매우 복잡하기 때문에 짧게 설명하기는 어렵습니다. 
앞에 작성한 예시로 정규 표현식의 구조만 간단히 살펴보겠습니다. IP 주소가 주어졌을 때 IP 주소의 형태가 맞는지 아닌지를 판단하기 어렵습니다. 
그러므로 앞에서처럼 up{instance!~"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}:.+$"} 같은 정규 표현식을 작성해
0~255 사이의 숫자가 점(.)으로 4개 연결되는 조건과 같은 패턴이 있는지 판단하고 이를 검색할 수 있습니다. 
정규 표현식은 IP 주소 외에도 규칙성이 있는 전화번호, 이메일 주소, 비밀번호 검사 등에 사용됩니다
```

<br>

- PromQL 연산자

PromQL 연산자(Operator)는 메트릭의 값을 이용한 여러 가지 활용 방법을 제공합니다. 활용법에 따라 크게 4가지로 구분됩니다.


• 비교 연산자: 레이블 매처와 유사한 형태이나 비교 대상이 숫자이므로 크기를 구분하는 조건들이 있습니다. 사용되는 연산자는 ==, !=, >, <, >=, <=입니다.

• 논리 연산자: 수집된 메트릭에서 보고 싶은 범위를 지정하는 and(교집합), or(합집합), unless(차집합) 연산자가 있습니다.

• 산술 연산자: 사칙연산(+, -, *, /), 나머지(%), 지수(^) 같은 연산자로 메트릭의 값을 사용자가 원하는 값으로 변환합니다.

• 집계 연산자: 평균(avg), 합계(sum), 계수(count)와 같이 수집된 메트릭을 종합하고 분석하는 연산자로 메트릭의 값을 좀 더 의미 있는 데이터로 정리합니다.

<br>

- PromQL 데이터 타입

쿼리 입력기에 node_cpu_seconds_total처럼 PromQL 표현에 맞는 쿼리를 입력해도 시간 정보는 나타나지 않습니다. 

메트릭 이름, 레이블, 메트릭 값만 표시될 뿐입니다. 사실 메트릭 데이터는 현재 시간을 내포하기 때문에 시간에 대한 정보가 결과에 표현되지 않을 뿐입니다.

시간을 포함한 메트릭 데이터를 확인하려면 쿼리를 입력할 때 [구간 값]을 입력해야 합니다. 

메트릭 데이터를 받아오는 구간을 프로메테우스에서는 `레인지 셀렉터(Range Selector)`라고 합니다. 

레인지 셀렉터를 설정하면 메트릭 값과 함께 시간 정보가 나타나 각 메트릭 값을 구분할 수 있습니다. 

앞에서 작성한 쿼리에 추가한 [5m]은 5분 동안 발생된 메트릭 값을 요청합니다. 

레인지 셀렉터의 단위로는 ms(밀리초), s(초), m(분), h(시간), d(일), w(주), y(년)를 사용할 수 있습니다.

이렇게 구간이 있는 PromQL 데이터 타입을 `레인지 벡터(Range vector)`라고 하고,

특정(일반적으로 현재 시점을 의미함) 시점에 대한 메트릭 값만을 가지는 PromQL 데이터 타입을 `인스턴트 벡터(Instant vector)`라고 합니다.

두 가지 타입 외에도 실수 값을 표현하는 `스칼라 타입(Scalar type)`과 문자열을 표현하는 `스트링 타입(String type)`이 있습니다. 

스칼라 타입은 주로 인스턴트 벡터 값을 변경하는 용도로 사용되고 단독으로 사용하지는 않습니다. 

스트링 타입은 프로메테우스 2.0부터는 사용하지 않습니다.

<br>

레인지 벡터와 인스턴트 벡터의 사용법을 알려면 메트릭 타입을 이해해야 합니다. 게이지 타입으로 수집된 가용 메모리의 메트릭 데이터를 그래프로 그리면 한눈에 상태 변화를 확인할 수 있습니다.

하지만 카운터 타입은 항상 증가하는 값이기 때문에 그래프로 그리면 계속 증가하는 형태로만 보입니다. 따라서 어떠한 의미도 알아낼 수 없습니다.

의미 있는 메트릭 데이터의 흐름을 보려면 게이지 타입의 메트릭을 사용해야 하는데, 메트릭 값에 따라 카운터 타입만을 제공하는 경우가 있습니다. 

대표적인 예가 노드의 총 CPU 사용 시간을 파악할 수 있는 node_cpu_seconds_total입니다. 

그래서 이런 경우에는 카운터 타입으로 구성된 레인지 벡터를 이용해 유의미한 결과를 만들어야 합니다. 

즉, 레인지 벡터로 구성된 값들의 변화를 계산해 변화율을 그래프 형태로 그리는 것입니다.


<br>

- PromQL 함수

![image](https://user-images.githubusercontent.com/62640332/211337885-88198e83-bb9d-4bfa-99a2-3d8c44dea2a8.png)

<br>

- 변화율을 나타내는 rate

rate는 수집한 값들의 변화율을 구할 때 사용합니다. 주로 값이 증가하는 카운터 형식의 메트릭에 사용되며, 지정된 구간이 얼마나 빠르게 변화했는지 알기 위한 지표로 사용됩니다.

ex) rate(node_cpu_seconds_total{mode="idle", kubernetes_node="w2-k8s"}[5m])

![image](https://user-images.githubusercontent.com/62640332/211338693-511b49bc-3923-4e35-a149-0f60aceb9e5a.png)

프로메테우스에서 제공하는 그래프는 몇 가지 단점이 있습니다.

• 변화율 값을 정확하게 표시하지 못합니다.

• 제한이 많은 꺾은선 그래프로 표시돼 추이 변화를 파악하기 어렵습니다.

• 하나의 패널로 구성돼 여러 가지의 PromQL 쿼리를 비교할 수 없습니다.

=> Grafana 사용 필요

<br>

- 순간변화율을 나타내는 irate

rate는 구간 시작 값과 구간 종료 값의 차이에 대한 변화율을 다루고, 

irate는 구간 종료 바로 전 값과 구간 종료 값의 차이에 대한 변화율을 나타내는 점이 다릅니다. 

따라서 구간이 매우 길면 irate의 변화율은 큰 의미가 없기 때문에 rate 함수를 사용하는 것이 낫습니다.

<br>

- 서머리

서머리는 이미 공개된 메트릭 값을 조회하면 바로 확인할 수 있습니다.

쿼리 입력기에 prometheus_target_interval_length_seconds를 입력합니다. 

결과에서 quantile(분위수)의 값에 매핑되는 메트릭 값을 확인합니다. 

프로메테우스 서버가 수집 대상으로부터 60.021674221초로 응답받았다면 이는 전체 100개 중에서 99번째(quantile="0.99") 응답 시간을 의미합니다.

<br>

- 히스토그램

히스토그램은 쿼리 입력기에서 쿼리를 보내면 그때서야 내부 계산식을 통해 히스토그램 메트릭을 생성합니다. 

쿼리 입력기에 histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket[5m]))을 입력합니다.

apiserver_request_duration_seconds_bucket은 API 서버의 응답 시간이고, 메트릭 값의 타입은 카운터입니다. 

여기서 rate 함수를 사용한 이유는 메트릭 값을 변화율로 바꾸려는 것이 아니라 히스토그램에서 관측하는 범위를 지정하기 위해서입니다.

그리고 0.99는 API 서버의 응답 시간이 99백분위수인 값을 찾습니다. 

참고로 검색된 값 중에 NaN(Not a Number)은 버킷(개발자가 지정해 둔 측정 범위)으로 검출된 값이 없을 때 표시되는 값입니다.

<br>

![image](https://user-images.githubusercontent.com/62640332/212552913-e75f12af-1bee-4aad-98c8-c9381355e8a6.png)

![image](https://user-images.githubusercontent.com/62640332/212552932-625c9bdc-0249-4d29-af78-ee38cab4e6a3.png)

<br>
<br>

### ㅁ 그라파나

- 시각화 옵션 list

그래프(Graph): 그라파나에서 가장 많이 사용되는 기본 옵션으로, 점의 경로, 선 또는 막대로 데이터를 시각화할 수 있습니다. 대부분의 시계열 데이터를 사용자가 원하는 형태로 표시합니다.

• 상태 값(Stat): 특정 값과 함께 추이를 꺾은선 그래프로 나타내는 도구입니다. 텍스트 모드를 적용해 그래프 없이 값만 볼 수도 있습니다. 주로 여러 대상이나 단일 대상의 상태를 나타내는 데 사용합니다.

• 게이지(Gauge): 차량의 계기판과 같이 값의 범위가 있는 데이터를 표현할 때 유용합니다. 주로 백분율처럼 사용량의 시작과 끝이 있을 때 사용합니다.

• 바 게이지(Bar gauge): 게이지처럼 값의 범위가 있는 데이터를 표현할 때 유용한데, 게이지와 다르게 모양이 원형이 아닌 막대형으로 표현됩니다. 게이지보다는 범위가 더 넓어 한계를 모르는 경우에 사용합니다.

• 테이블(Table): 수집된 메트릭 데이터를 그대로 표현할 수 있어서 주로 실제 값을 대시보드에서 확인하는 목적으로 사용합니다.

• 텍스트(Text): 제작자가 알리고 싶은 정보를 대시보드에 표현할 때 사용됩니다. 주로 대시보드의 목적과 사용법 등의 정보를 전달하는 데 사용합니다.

• 히트맵(Heatmap): 패널을 다수 구역으로 나누어 해당 구역에 속하는 값이 많을수록 구역 색상을 점점 연하게 표현해 한눈에 값의 분포를 알아볼 수 있습니다. 주로 히스토그램과 함께 빈도를 나타내는 용도로 사용합니다.

• 경보 목록(Alert list): 수집 대상의 문제를 빠르게 확인하기 위한 정보를 표시합니다. 가장 최근에 발생한 경보부터 확인할 수 있습니다.

• 대시보드 목록(Dashboard list): 최근에 확인한 대시보드와 사용자가 즐겨찾기한 대시보드 등을 만드는 데 사용합니다.

• 뉴스(News): RSS 피드와 같은 정보를 나타냅니다. 기본적으로 그라파나 공식 블로그의 게시글 정보를 표시합니다.

• 로그(Logs): 외부 로그 데이터를 나타냅니다. 주로 일래스틱서치, 로키(Loki), 인플럭스DB 같은 데이터 소스에서 받아온 로그 데이터를 시각화하는 데 사용합니다. 추가로 로그 수준(level)에 따라 패널에 색상 변화를 넣을 수 있습니다.

• 플러그인 목록(Plugin list): 현재 설치된 플러그인을 확인합니다. 그라파나에서 기본으로 제공하는 패널만으로는 부족한 기능을 외부에서 제공하는 플러그인 방식의 패널로 추가하는데, 이렇게 추가된 패널들을 대시보드에서 확인할 수 있게 합니다. 패널 외에도 플러그인을 사용하면 기존에 그라파나에서 제공하지 않던 다양한 기능도 추가할 수 있습니다.

<br>

- Setting 의 Variables 설정 항목

➊ name: 대시보드에서 사용하는 변수 이름으로 여기서는 Namespace를 입력합니다. 대시보드에서 $Namespace로 변수를 사용할 수 있습니다.

➋ Label: 대시보드에서 변수 선택 시 변수를 지칭하는 레이블입니다. 변수 이름과 같게 Namespace로 설정합니다.

➌ Data Source: ➎ 의 쿼리가 실행될 때 값을 받아오는 소스를 설정합니다. 노드 메트릭을 가지고 오는 곳과 동일하게 Prometheus를 선택합니다.

➍ Refresh: 변수를 읽어 들이는 방법을 설정합니다. 대시보드가 로드될 때마다 새로 읽어 들이도록 On Dashboard Load를 선택합니다.

➎ Query: 쿼리를 입력해 결과를 그라파나 변수로 사용하도록 추가합니다. 입력한 label_values(kube_pod_info, namespace)에서 label_values는 프로메테우스 플러그인에서 제공하는 함수로, 메트릭에 있는 특정 레이블의 값을 반환받을 수 있습니다. 현재 쿼리는 kube_pod_info의 namespace 값을 그라파나의 Namespace 변수의 값으로 치환합니다.
➏ Include All option: 모든 네임스페이스를 선택할 수 있는 옵션을 적용할지 말지를 결정합니다. 스위치를 활성화하면 네임스페이스를 모두 선택할 수 있는 All 선택 옵션이 추가됩니다.

➐ Custom all value: ➏ 에서 스위치를 활성화했을 때 추가되는 옵션으로, All 선택 옵션의 범위를 사용자가 지정할 수 있습니다. .+를 설정하면 하나 이상의 값을 가진 것들을 선택할 수 있습니다.


설정 항목을 모두 작성하면 변수로 사용할 수 있는 값이 Preview of values 항목 밑에 모두 표시됩니다. 이를 확인하고 Add 버튼을 눌러 변수 설정을 완료합니다.

```
Tip ☆ SLI, SLO, SLA
    
사이트 신뢰성 엔지니어링(SRE, Site Reliability Engineering) 영역에서는 서비스 수준 지표(SLI, Service Level Indicator)를 기준으로 서비스 수준 목표(SLO, Service Level Objective)를 산정할 수 있습니다. 
예를 들어 API 서버에 대한 요청부터 응답 시간(SLI)이 2분 이내여야 한다는 목표(SLO)가 있다면 응답 시간 5분 지연 시 월 서비스 사용료에서 2%를 할인한다는 서비스 수준 계약(SLA, Service Level Agreement)을 체결할 수 있습니다. 
SLA를 적용함으로써 서비스 제공자는 서비스의 범위를 정할 수 있고, 사용자는 SLA를 기준으로 서비스 품질을 정량적으로 측정할 수 있습니다.
```

<br>

- alert manager

• alert: 경보 메시지를 대표하는 이름

• expr: PromQL 표현식으로 경보 규칙을 설정해 참인 경우 경보 발생

• for: 문제 지속 시간

• annotations: 하위 항목에 필요한 전달 메시지를 주석으로 기록

• description: 사용자 임의 지정 변수로 필요한 경보 내용을 작성할 수 있음